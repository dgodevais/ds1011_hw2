{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-1011 NLP HW 2 Code Base\n",
    "\n",
    "## Part 1. Generating Text Vectors and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# dtype = torch.FloatTensor\n",
    "dtype_float = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "dtype_long = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary: you can call line.split() to immediately get the tokenized version of \n",
    "# the sentences. Normally you would have to run the text through a proper tokenizer.\n",
    "\"\"\"\n",
    "You should create a subclass for Dataset, but not for Dataloader. \n",
    "You should probably also define a custom collate_fn, \n",
    "but you should also only need to do this once.\n",
    "You will instantiate separate Datasets: e.g. train_dataset and val_dataset. \n",
    "Then you should create a Dataloader around each one: train_dataloader and val_dataloader.\n",
    "For your max length, that should be determined based on your training set \n",
    "(you should pretend you do not have access to your validation set when choosing \n",
    "such \"hyperparameters\", to avoid biasing your results.\n",
    "\n",
    "sent1_vector = rnn(sent1)\n",
    "sent2_vector = rnn(sent2)\n",
    "combined_vector = torch.cat([sent1_vector, sent2_vector], dim=1)\n",
    "\n",
    "nn.Sequential(nn.Linear(x, hidden_size), \n",
    "              nn.ReLU(inplace=True), \n",
    "              nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "You are required to concatenate the representations, not the sentences. \n",
    "The idea is that you will encode each of your sentences through an \n",
    "encoder (CNN or RNN) and extract a fixed-length vector representation \n",
    "of the sentences. You will then concatenate the two representations and \n",
    "feed that through a fully-connected layer.\n",
    "\n",
    "Transfer the string into word vectors using fast text, process \n",
    "them with data loader, feed them separately into the encoder, \n",
    "get the output representation, concat them and then fed that into a \n",
    "fully-connected layer for classification.\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelDatasetBuilder(object):\n",
    "    \"\"\"\n",
    "    Use this class to build the datasets for model consumption\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, vocab_size=10000):\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.TKN_OFFSET = 2\n",
    "        self.EMB_SIZE = 300\n",
    "        self.vocab_size = vocab_size\n",
    "        self.id2token = []\n",
    "        self.token2id = None\n",
    "        self.fasttext_emb_map = {}\n",
    "        pass\n",
    "    \n",
    "    def load_fasttext_vectors_into_vocabulary(self, fname):\n",
    "        self.id2token = [None] * (self.vocab_size + self.TKN_OFFSET)\n",
    "        with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n",
    "            n, d = map(int, fin.readline().split())\n",
    "            for i, line in enumerate(fin):\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                self.id2token[i + self.TKN_OFFSET] = tokens[0]\n",
    "                self.fasttext_emb_map[tokens[0]] = list(map(float, tokens[1:]))\n",
    "                if i>self.vocab_size - self.TKN_OFFSET:\n",
    "                    break\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "        self.id2token[self.PAD_IDX] = '<pad>'\n",
    "        self.id2token[self.UNK_IDX] = '<unk>'\n",
    "        self.token2id = dict(zip(self.id2token, range(0,len(self.id2token)))) \n",
    "        self.token2id['<pad>'] = self.PAD_IDX \n",
    "        self.token2id['<unk>'] = self.UNK_IDX\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "    \n",
    "    def get_indexed_data(self, data_path, max_data, mnli=False):\n",
    "        label_mapper = {\n",
    "            \"contradiction\\n\": 0,\n",
    "            \"contradiction\": 0,\n",
    "            \"entailment\\n\": 1,\n",
    "            \"entailment\": 1,\n",
    "            \"neutral\\n\": 2,\n",
    "            \"neutral\": 2\n",
    "        }\n",
    "        lines = []\n",
    "        with open(data_path, 'r', newline='\\n') as f:\n",
    "            next(f)\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                parts = line.split('\\t')\n",
    "                premise_tokens =  parts[0].split()\n",
    "                premise_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in premise_tokens]\n",
    "                hypothesis_tokens =  parts[1].split()\n",
    "                hypothesis_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in hypothesis_tokens]\n",
    "                label =  label_mapper[parts[2]]\n",
    "                if mnli:\n",
    "                    cat = parts[3]\n",
    "                    row = premise_indices, hypothesis_indices, label, cat\n",
    "                else:\n",
    "                    row = premise_indices, hypothesis_indices, label\n",
    "                lines.append( row )\n",
    "                if i > max_data:\n",
    "                    break\n",
    "                i += 1\n",
    "        print(\"All of the lines!!!\")\n",
    "        print(len(lines))\n",
    "        return lines\n",
    "    \n",
    "    def get_embedding_vector(self):\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        pre_trained_embeddings = []\n",
    "        for i in range(0, self.vocab_size):\n",
    "            token = self.id2token[i]\n",
    "            pre_trained_embeddings.append(self.fasttext_emb_map[token])\n",
    "#         weight = torch.FloatTensor(pre_trained_embeddings)\n",
    "        weight = torch.FloatTensor(pre_trained_embeddings).type(dtype_float).cuda()\n",
    "#         weight = weight.long()\n",
    "        embedding = nn.Embedding.from_pretrained(weight, freeze=True).cuda()\n",
    "        return embedding\n",
    "    \n",
    "    def get_indexed_text_vectors(self, max_data=float(\"inf\")):\n",
    "        \"\"\"\n",
    "        Gets the torch.utils.data.Dataset preproccessed version\n",
    "        \"\"\"\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        training_vectors = self.get_indexed_data('hw2_data/snli_train.tsv', max_data)\n",
    "        val_vectors = self.get_indexed_data('hw2_data/snli_val.tsv', max_data)\n",
    "        return training_vectors, val_vectors\n",
    "    \n",
    "    def get_mnli_indexed_text_vectors(self, max_data=float(\"inf\")):\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        val_mnli_vectors = self.get_indexed_data('hw2_data/mnli_val.tsv', max_data, mnli=True)\n",
    "        train_mnli_vectors = self.get_indexed_data('hw2_data/mnli_train.tsv', max_data, mnli=True)\n",
    "        return train_mnli_vectors, val_mnli_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "class SnliDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, premises, hypotheses, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.premises = premises\n",
    "        self.hypotheses = hypotheses\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.premises) == len(self.target_list))\n",
    "        assert (len(self.hypotheses) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        premise_idx = self.premises[index][:MAX_SENTENCE_LENGTH]\n",
    "        hypothesis_idx = self.hypotheses[index][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[index]\n",
    "        return [premise_idx, hypothesis_idx, len(premise_idx), len(hypothesis_idx), label]\n",
    "    \n",
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    premise_list = []\n",
    "    hypothesis_list = []\n",
    "    label_list = []\n",
    "    premise_length_list = []\n",
    "    hypothesis_length_list = []\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    for datum in sorted_batch:\n",
    "        label_list.append(datum[4])\n",
    "        premise_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "        hypothesis_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "    # padding\n",
    "    for datum in sorted_batch:\n",
    "        padded_premise_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        premise_list.append(padded_premise_vec)\n",
    "        padded_hypothesis_vec = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        hypothesis_list.append(padded_hypothesis_vec)\n",
    "\n",
    "    collation = [torch.FloatTensor(premise_list).type(dtype_long), \n",
    "                 torch.FloatTensor(hypothesis_list).type(dtype_long), \n",
    "                 torch.FloatTensor(hypothesis_length_list).type(dtype_long), \n",
    "                 torch.FloatTensor(label_list).type(dtype_long)]\n",
    "    return collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of questions\n",
    "\"\"\"\n",
    "- Do I need to instantiate separate versions of the model for each premise and hypothesis\n",
    "both linear and RNN\n",
    "- Do I have to instantiate in the init function or can I do it in the forward for things\n",
    "like the nn.Sequential operator\n",
    "- How do I parallelize it?\n",
    "- How does the conv net work?\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes, element_wise=False):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size        \n",
    "        super(GRU, self).__init__()\n",
    "        print(\"GRU Settings...\")\n",
    "        print(\"embeddings: \" + str(embeddings.embedding_dim))\n",
    "        print(\"hidden size: \" + str(hidden_size))\n",
    "        print(\"num layers: \" + str(num_layers))\n",
    "        print(\"num classes: \" + str(num_classes))\n",
    "        self.num_layers, self.hidden_size, self.element_wise = num_layers, hidden_size, element_wise\n",
    "        self.embedding = embeddings.cuda()\n",
    "        self.rnn_premise = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.rnn_hypothesis = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.linear_premise = nn.Linear(hidden_size, num_classes)\n",
    "        self.linear_hyp = nn.Linear(hidden_size, num_classes)\n",
    "        if element_wise:\n",
    "            fully_connected_dim = 128\n",
    "            self.linear_fully_connected = nn.Linear(3, fully_connected_dim)\n",
    "        else:\n",
    "            fully_connected_dim = 32\n",
    "            self.linear_fully_connected = nn.Linear(6, fully_connected_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(fully_connected_dim, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = premises.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Run encoder for the premise\n",
    "        # get embedding of tokens\n",
    "        embed_premises = self.embedding(premises)\n",
    "        # pack padded sequence\n",
    "        embed_premises = torch.nn.utils.rnn.pack_padded_sequence(embed_premises, \n",
    "                                                                 lengths, \n",
    "                                                                 batch_first=True)\n",
    "        # fprop though RNN\n",
    "        rnn_out_prem, self.hidden = self.rnn_premise(embed_premises, self.hidden)\n",
    "        # undo packing\n",
    "        rnn_out_prem, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_prem, \n",
    "                                                                 batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out_prem = torch.sum(rnn_out_prem, dim=1)\n",
    "        logits_prem = self.linear_premise(rnn_out_prem)\n",
    "\n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "#         embed_hyp = torch.nn.utils.rnn.pack_padded_sequence(embed_hyp, \n",
    "#                                                                  lengths.numpy(), \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp, self.hidden = self.rnn_hypothesis(embed_hyp, self.hidden)\n",
    "#         rnn_out_hyp, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_hyp, \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp = torch.sum(rnn_out_hyp, dim=1)\n",
    "        logits_hyp = self.linear_hyp(rnn_out_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        if self.element_wise:\n",
    "#             print(logits_prem.size())\n",
    "#             print(logits_hyp.size())\n",
    "            combined_sentences = torch.mul(logits_prem, logits_hyp)\n",
    "#             print(combined_sentences.size())\n",
    "        else:\n",
    "            combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes, kernel_size=3, element_wise=False):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = embeddings.cuda()\n",
    "        self.element_wise = element_wise\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(embeddings.embedding_dim, hidden_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=1)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool1d(kernel_size, stride=2)\n",
    "        self.linear = nn.Linear( int(np.ceil(hidden_size / 2.0) - 1), 128)\n",
    "        #128 x 256\n",
    "        if element_wise:\n",
    "            fully_connected_dim = 128\n",
    "            self.linear_fully_connected = nn.Linear(3, fully_connected_dim)\n",
    "        else:\n",
    "            fully_connected_dim = 256\n",
    "            self.linear_fully_connected = nn.Linear(6, fully_connected_dim)\n",
    "        self.linear_fully_connected = nn.Linear(fully_connected_dim, 128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        premises, hypotheses, lengths = premises.cuda(), hypotheses.cuda(), lengths.cuda()\n",
    "        \n",
    "        batch_size, seq_len = premises.size()\n",
    "        \n",
    "        # Run encoder for the premises\n",
    "#         print(\"DEGUB-----\")\n",
    "#         print(premises)\n",
    "#         print(premises.double())\n",
    "#         print(premises.int())\n",
    "#         print(premises.type(torch.cuda.LongTensor))\n",
    "        embed_premises = self.embedding(premises)\n",
    "        hidden_prem = self.conv1(embed_premises.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "\n",
    "        hidden_prem = self.conv2(hidden_prem.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "        \n",
    "        pooled_prem = self.max_pool(hidden_prem)\n",
    "\n",
    "        hidden_prem = torch.sum(pooled_prem, dim=1)\n",
    "        logits_prem = self.linear(hidden_prem)\n",
    "        \n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "        hidden_hyp = self.conv1(embed_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "\n",
    "        hidden_hyp = self.conv2(hidden_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "        \n",
    "        pooled_hyp = self.max_pool(hidden_hyp)\n",
    "\n",
    "        hidden_hyp = torch.sum(pooled_hyp, dim=1)\n",
    "        logits_hyp = self.linear(hidden_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        if self.element_wise:\n",
    "            combined_sentences = torch.mul(logits_prem, logits_hyp)\n",
    "        else:\n",
    "            combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Model Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def run_model(mdb, model, accuracies, learning_rate, num_epochs, train_loader, val_loader, prefix):\n",
    "    model.cuda()\n",
    "    print(\"Running the model\")\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    start = time()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (premises, hypotheses, lengths, labels) in enumerate(train_loader):\n",
    "#             print(\"MADE IT HERE???\")\n",
    "#             print(premises)\n",
    "#             print(labels)\n",
    "#             print(i)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(premises, hypotheses, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "#             print(loss)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 30 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc))\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                end = time()\n",
    "                print(\"Elapsed time {} seconds\".format(round(end - start, 2)))\n",
    "                start = time()\n",
    "                accuracies.append( (epoch+1, i, train_acc, val_acc, model.hidden_size, model.element_wise) )\n",
    "                if val_acc > 69:\n",
    "                    pkl.dump(accuracies, open(\"{}_accuracies5_{}_{}\".format(prefix, model.hidden_size, model.element_wise), \"wb\"))\n",
    "                    torch.save(model, \n",
    "                               \"{}_best_model_{}_{}.pt\".format(prefix, model.hidden_size, model.element_wise))\n",
    "                    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdb = ModelDatasetBuilder('hw2_data', vocab_size=50000)\n",
    "mdb.load_fasttext_vectors_into_vocabulary('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50000, 300)\n",
      "All of the lines!!!\n",
      "100000\n",
      "All of the lines!!!\n",
      "1000\n",
      "[[106, 802, 1830, 8, 9, 6265, 7167, 4388, 17, 9, 12229, 5335, 9, 563, 6, 358, 4], [106, 994, 15, 9766, 131, 3, 347, 424, 7, 86, 15, 3356, 17, 9, 2031, 1615, 19, 9, 2607, 17, 21, 4], [17790, 3, 1938, 2, 32, 84, 138, 9, 542, 402, 4], [9, 347, 8, 909, 11907, 5, 9, 884, 7167, 15, 1, 17, 3, 5656], [3543, 884, 3135, 441, 549, 17, 4808, 1826, 5472, 4]]\n",
      "[2, 1, 2, 0, 0]\n",
      "hidden size 200\n",
      "element wise: False\n",
      "Running the model\n",
      "Epoch: [1/10], Step: [101/782], Training Acc: 51.651\n",
      "Epoch: [1/10], Step: [101/782], Validation Acc: 51.4\n",
      "Elapsed time 14.65 seconds\n",
      "Epoch: [1/10], Step: [201/782], Training Acc: 56.94\n",
      "Epoch: [1/10], Step: [201/782], Validation Acc: 57.0\n",
      "Elapsed time 14.75 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-76c01dd61755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mrun_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-76c01dd61755>\u001b[0m in \u001b[0;36mrun_scenario\u001b[0;34m(mbd)\u001b[0m\n\u001b[1;32m     53\u001b[0m                         element_wise=elmt_wise)\n\u001b[1;32m     54\u001b[0m             \u001b[0maccuracies_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cnn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_accuracies4_{}_{}_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melmt_wise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-63771507cd9f>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(mdb, model, accuracies, learning_rate, num_epochs, train_loader, val_loader, prefix)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {}'.format(\n\u001b[1;32m     51\u001b[0m                            epoch+1, num_epochs, i+1, len(train_loader), train_acc))\n",
      "\u001b[0;32m<ipython-input-40-63771507cd9f>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_scenario(mbd):\n",
    "    embeddings = mdb.get_embedding_vector()\n",
    "    print(embeddings)\n",
    "    training_vectors, val_vectors = mdb.get_indexed_text_vectors(max_data=1000000)\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    print([x[0] for x in training_vectors][0:5])\n",
    "    print([x[2] for x in training_vectors][0:5])\n",
    "    # print(training_vectors[0].size)\n",
    "    # print(training_vectors[2])\n",
    "    train_dataset = SnliDataset([x[0] for x in training_vectors], \n",
    "                                [x[1] for x in training_vectors], \n",
    "                                [x[2] for x in training_vectors])\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    val_dataset = SnliDataset([x[0] for x in val_vectors], \n",
    "                                [x[1] for x in val_vectors], \n",
    "                                [x[2] for x in val_vectors])\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    num_epochs = 10\n",
    "#     hidden_layers = [25, 50, 100, 200, 400]\n",
    "#     interactions = [True, False]\n",
    "    interactions = [False]\n",
    "    hidden_layers = [200]\n",
    "    for layer in hidden_layers:\n",
    "        print(\"hidden size \"+ str(layer))\n",
    "        for elmt_wise in interactions:\n",
    "            print(\"element wise: \" + str(elmt_wise))\n",
    "#             gru_learning_rate = 1e-3\n",
    "#             model_gru = GRU(embeddings=embeddings, \n",
    "#                         hidden_size=layer, \n",
    "#                         num_layers=2, \n",
    "#                         num_classes=3,\n",
    "#                         element_wise=elmt_wise)\n",
    "#             accuracies_gru = []\n",
    "#             run_model(mdb, model_gru, accuracies_gru, gru_learning_rate, num_epochs, train_loader, val_loader, 'gru')\n",
    "#             pkl.dump(accuracies_gru, \n",
    "#                      open(\"gru_accuracies4_{}_{}_{}\".format(gru_learning_rate,layer, elmt_wise), \"wb\"))\n",
    "#             print(accuracies_gru)\n",
    "        \n",
    "            cnn_learning_rate = 3e-4\n",
    "            model_cnn = CNN(embeddings=embeddings, \n",
    "                        hidden_size=layer, \n",
    "                        num_layers=2, \n",
    "                        num_classes=3,\n",
    "                        element_wise=elmt_wise)\n",
    "            accuracies_cnn = []\n",
    "            run_model(mdb, model_cnn, accuracies_cnn, cnn_learning_rate, num_epochs, train_loader, val_loader, 'cnn')\n",
    "            pkl.dump(accuracies_cnn, open(\"cnn_accuracies4_{}_{}_{}\".format(cnn_learning_rate,layer,elmt_wise), \"wb\"))\n",
    "            print(accuracies_cnn)\n",
    "\n",
    "run_scenario(mdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden size 200\n",
      "GRU Settings...\n",
      "embeddings: 300\n",
      "hidden size: 200\n",
      "num layers: 2\n",
      "num classes: 3\n",
      "1086905\n",
      "330099\n",
      "hidden size 200\n",
      "GRU Settings...\n",
      "embeddings: 300\n",
      "hidden size: 200\n",
      "num layers: 2\n",
      "num classes: 3\n",
      "1086329\n",
      "346483\n",
      "[(1086905, 200), (330099, 200), (1086329, 200), (346483, 200)]\n"
     ]
    }
   ],
   "source": [
    "def get_num_model_params(model, layer):\n",
    "    # https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "    return pytorch_total_params, layer\n",
    "\n",
    "# hidden_layers = [25, 50, 100, 200, 400]\n",
    "hidden_layers = [200]\n",
    "params = []\n",
    "wises = [True, False]\n",
    "for layer in hidden_layers:\n",
    "    for elmt_wise in wises:\n",
    "        print(\"hidden size \"+ str(layer))\n",
    "        gru_learning_rate = 1e-3\n",
    "        model_gru = GRU(embeddings=mdb.get_embedding_vector(), \n",
    "                    hidden_size=layer, \n",
    "                    num_layers=2, \n",
    "                    num_classes=3,\n",
    "                    element_wise=elmt_wise)\n",
    "        params.append(get_num_model_params(model_gru, layer))\n",
    "        model_cnn = CNN(embeddings=mdb.get_embedding_vector(), \n",
    "                        hidden_size=layer, \n",
    "                        num_layers=2, \n",
    "                        num_classes=3,\n",
    "                       element_wise=elmt_wise)\n",
    "        params.append(get_num_model_params(model_cnn, layer))\n",
    "\n",
    "print(params) # 1086329\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the lines!!!\n",
      "100000\n",
      "All of the lines!!!\n",
      "1000\n",
      "\n",
      "CORRECT\n",
      "Premise: Four people sit on a subway two read books , one looks at a cellphone and is wearing knee high boots .\n",
      "Hypothesis: Multiple people are on a subway together , with each of them doing their own thing .\n",
      "Label: 1\n",
      "Premise: bicycles stationed while a group of people socialize .\n",
      "Hypothesis: People get together near a stand of bicycles .\n",
      "Label: 1\n",
      "Premise: Two people are in a green forest .\n",
      "Hypothesis: The forest is not dead .\n",
      "Label: 1\n",
      "Premise: Two women , one walking her dog the other pushing a <unk> .\n",
      "Hypothesis: There is a snowstorm .\n",
      "Label: 0\n",
      "\n",
      "INCORRECT\n",
      "Premise: Three women on a stage , one wearing red shoes , black pants , and a gray shirt is sitting on a prop , another is sitting on the floor , and the third wearing a black shirt and pants is standing , as a gentleman in the back tunes an instrument .\n",
      "Hypothesis: Three women on a stage , one wearing red shoes , black pants , and a gray shirt is sitting on a prop , another is sitting on the floor , and the third wearing a black shirt and pants is standing , as a gentleman in the back tunes an instrument .\n",
      "Label: 0\n",
      "Premise: Man in <unk> with two horses .\n",
      "Hypothesis: Man in <unk> with two horses .\n",
      "Label: 1\n",
      "Premise: Man observes a wavelength given off by an electronic device .\n",
      "Hypothesis: Man observes a wavelength given off by an electronic device .\n",
      "Label: 1\n",
      "Premise: Two men are listening to music through headphones .\n",
      "Hypothesis: Two men are listening to music through headphones .\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_from_vector(mdb, vector):\n",
    "    sentence = []\n",
    "    for i in vector:\n",
    "        sentence.append(mdb.id2token[i])\n",
    "    return \" \".join(sentence)\n",
    "        \n",
    "\n",
    "def get_correct_and_incorrect_predictions(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        correct_list = []\n",
    "        incorrect_list = []\n",
    "        pairs = zip(predicted.squeeze(1).tolist(), label_batch.tolist())\n",
    "        for i, pair in enumerate(pairs):\n",
    "            pred, label = pair\n",
    "            if (len(correct_list) > 3) and (len(incorrect_list) > 3):\n",
    "                return correct_list, incorrect_list\n",
    "            if pred != label:\n",
    "                incorrect_list.append(i)\n",
    "            else:\n",
    "                correct_list.append(i)\n",
    "        return correct_list, incorrect_list\n",
    "\n",
    "\n",
    "training_vectors, val_vectors = mdb.get_indexed_text_vectors(max_data=1000000)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "val_dataset = SnliDataset([x[0] for x in val_vectors], \n",
    "                            [x[1] for x in val_vectors], \n",
    "                            [x[2] for x in val_vectors])\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)\n",
    "    \n",
    "best_model = torch.load(\"cnn_best_model_200_False.pt\")\n",
    "correct, incorrect = get_correct_and_incorrect_predictions(val_loader, best_model)\n",
    "print()\n",
    "print(\"CORRECT\")\n",
    "for correct_i in correct:\n",
    "    print(\"Premise: {}\".format(get_sentence_from_vector(mdb, val_vectors[correct_i][0])))\n",
    "    print(\"Hypothesis: {}\".format(get_sentence_from_vector(mdb, val_vectors[correct_i][1])))\n",
    "    print(\"Label: {}\".format(val_vectors[correct_i][2]))\n",
    "print()\n",
    "print(\"INCORRECT\")\n",
    "for incorrect_i in incorrect:\n",
    "    print(\"Premise: {}\".format(get_sentence_from_vector(mdb, val_vectors[incorrect_i][0])))\n",
    "    print(\"Hypothesis: {}\".format(get_sentence_from_vector(mdb, val_vectors[incorrect_i][0])))\n",
    "    print(\"Label: {}\".format(val_vectors[incorrect_i][2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Plotting (see \"DS1011_NLP_HW2_graphing\" notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this so your plots show properly\n",
    "# See rest of graph generation code in other notebook \"DS1011_NLP_HW2_graphing\"\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "def get_plot_values(accuracies):\n",
    "    training_acc = [acc[2] for acc in accuracies]\n",
    "    val_acc = [acc[3] for acc in accuracies]\n",
    "    training_acc_fmt = []\n",
    "    for i, acc in enumerate(training_acc):\n",
    "        if i % 6 == 0:\n",
    "            continue\n",
    "        elif i % 7 == 0:\n",
    "            continue\n",
    "        else:\n",
    "            training_acc_fmt.append(acc)\n",
    "    val_acc_fmt = []\n",
    "    for i, acc in enumerate(val_acc):\n",
    "        if i % 6 == 0:\n",
    "            continue\n",
    "        elif i % 7 == 0:\n",
    "            continue\n",
    "        else:\n",
    "            val_acc_fmt.append(acc)\n",
    "    return training_acc_fmt, val_acc_fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 5. Evaluating on MultiNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the lines!!!\n",
      "5000\n",
      "All of the lines!!!\n",
      "20000\n",
      "\n",
      "GENRE: slate\n",
      "\n",
      "contains 1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:56: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:69: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acc for GRU is 44.11177644710579\n",
      "The acc for CNN is 42.81437125748503\n",
      "\n",
      "GENRE: fiction\n",
      "\n",
      "contains 995\n",
      "The acc for GRU is 44.824120603015075\n",
      "The acc for CNN is 44.321608040201006\n",
      "\n",
      "GENRE: government\n",
      "\n",
      "contains 1016\n",
      "The acc for GRU is 45.17716535433071\n",
      "The acc for CNN is 43.503937007874015\n",
      "\n",
      "GENRE: telephone\n",
      "\n",
      "contains 1005\n",
      "The acc for GRU is 47.76119402985075\n",
      "The acc for CNN is 49.45273631840796\n",
      "\n",
      "GENRE: travel\n",
      "\n",
      "contains 982\n",
      "The acc for GRU is 46.94501018329939\n",
      "The acc for CNN is 44.09368635437882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntorch.save(the_model, PATH)\\n\\nmodel = Classifier()\\ntorch.save(model.state_dict(), ‘./model_Q2.pth’)\\nmodel.load_state_dict(torch.load(’./model_Q2.pth’, map_location=lambda storage, loc: storage))\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_mnli(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def run_mnli_scenario(mdb):\n",
    "#     embeddings = mdb.get_embedding_vector()\n",
    "#     print(embeddings)\n",
    "    train_mnli_vectors, val_vectors = mdb.get_mnli_indexed_text_vectors()\n",
    "#     genres = list(set([x[3] for x in val_vectors]))\n",
    "    genres = {}\n",
    "    for premise, hypothesis, label, genre in val_vectors:\n",
    "        if genre not in genres:\n",
    "            genres[genre] = [(premise, hypothesis, label)]\n",
    "        else:\n",
    "            genres[genre] = genres[genre] + [(premise, hypothesis, label)]\n",
    "    \n",
    "    for genre in genres.keys():\n",
    "        print()\n",
    "        print(\"GENRE: \" + genre)\n",
    "        print(\"contains \" + str(len(genres[genre])))\n",
    "\n",
    "        BATCH_SIZE = 128\n",
    "# #         print([x[0] for x in genres[genre]][0:5])\n",
    "#         print()\n",
    "#         print([x[2] for x in genres[genre]][0:5])\n",
    "\n",
    "        # print(training_vectors[0].size)\n",
    "        # print(training_vectors[2])\n",
    "        dataset = SnliDataset([x[0] for x in genres[genre]], \n",
    "                              [x[1] for x in genres[genre]], \n",
    "                              [x[2] for x in genres[genre]])\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=snli_collate_func,\n",
    "                                                   shuffle=True)\n",
    "        model = torch.load(\"gru_best_model_200_False.pt\")\n",
    "        val_acc = test_model(val_loader, model)\n",
    "        print(\"The acc for GRU is {}\".format(val_acc))\n",
    "        model = torch.load(\"cnn_best_model_200_False.pt\")\n",
    "        val_acc = test_model(val_loader, model)\n",
    "        print(\"The acc for CNN is {}\".format(val_acc))\n",
    "    return genres.keys()\n",
    "\n",
    "    \n",
    "run_mnli_scenario(mdb)\n",
    "\"\"\"\n",
    "torch.save(the_model, PATH)\n",
    "\n",
    "model = Classifier()\n",
    "torch.save(model.state_dict(), ‘./model_Q2.pth’)\n",
    "model.load_state_dict(torch.load(’./model_Q2.pth’, map_location=lambda storage, loc: storage))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Fine-tuning on MultiNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the lines!!!\n",
      "5000\n",
      "All of the lines!!!\n",
      "20000\n",
      "\n",
      "GENRE: slate\n",
      "\n",
      "Running the model\n",
      "Epoch: [1/8], Step: [31/32], Training Acc: 51.71385991058122\n",
      "Epoch: [1/8], Step: [31/32], Validation Acc: 47.604790419161674\n",
      "Elapsed time 1.23 seconds\n",
      "Epoch: [2/8], Step: [31/32], Training Acc: 58.1718827620467\n",
      "Epoch: [2/8], Step: [31/32], Validation Acc: 50.29940119760479\n",
      "Elapsed time 1.23 seconds\n",
      "Epoch: [3/8], Step: [31/32], Training Acc: 63.31346249379036\n",
      "Epoch: [3/8], Step: [31/32], Validation Acc: 49.00199600798403\n",
      "Elapsed time 1.22 seconds\n",
      "Epoch: [4/8], Step: [31/32], Training Acc: 68.80278191753601\n",
      "Epoch: [4/8], Step: [31/32], Validation Acc: 50.399201596806385\n",
      "Elapsed time 1.22 seconds\n",
      "Epoch: [5/8], Step: [31/32], Training Acc: 75.68306010928961\n",
      "Epoch: [5/8], Step: [31/32], Validation Acc: 48.902195608782435\n",
      "Elapsed time 1.22 seconds\n",
      "Epoch: [6/8], Step: [31/32], Training Acc: 82.66269249875808\n",
      "Epoch: [6/8], Step: [31/32], Validation Acc: 48.80239520958084\n",
      "Elapsed time 1.23 seconds\n",
      "Epoch: [7/8], Step: [31/32], Training Acc: 87.08395429706906\n",
      "Epoch: [7/8], Step: [31/32], Validation Acc: 48.50299401197605\n",
      "Elapsed time 1.23 seconds\n",
      "Epoch: [8/8], Step: [31/32], Training Acc: 93.02036761053155\n",
      "Epoch: [8/8], Step: [31/32], Validation Acc: 49.30139720558882\n",
      "Elapsed time 1.22 seconds\n",
      "[(1, 30, 51.71385991058122, 47.604790419161674, 200, False), (2, 30, 58.1718827620467, 50.29940119760479, 200, False), (3, 30, 63.31346249379036, 49.00199600798403, 200, False), (4, 30, 68.80278191753601, 50.399201596806385, 200, False), (5, 30, 75.68306010928961, 48.902195608782435, 200, False), (6, 30, 82.66269249875808, 48.80239520958084, 200, False), (7, 30, 87.08395429706906, 48.50299401197605, 200, False), (8, 30, 93.02036761053155, 49.30139720558882, 200, False)]\n",
      "\n",
      "GENRE: fiction\n",
      "\n",
      "Running the model\n",
      "[]\n",
      "\n",
      "GENRE: government\n",
      "\n",
      "Running the model\n",
      "Epoch: [1/8], Step: [31/31], Training Acc: 58.588720061807884\n",
      "Epoch: [1/8], Step: [31/31], Validation Acc: 53.1496062992126\n",
      "Elapsed time 1.17 seconds\n",
      "Epoch: [2/8], Step: [31/31], Training Acc: 63.45609065155807\n",
      "Epoch: [2/8], Step: [31/31], Validation Acc: 55.610236220472444\n",
      "Elapsed time 1.17 seconds\n",
      "Epoch: [3/8], Step: [31/31], Training Acc: 69.12181303116148\n",
      "Epoch: [3/8], Step: [31/31], Validation Acc: 54.82283464566929\n",
      "Elapsed time 1.17 seconds\n",
      "Epoch: [4/8], Step: [31/31], Training Acc: 73.96343033736801\n",
      "Epoch: [4/8], Step: [31/31], Validation Acc: 54.232283464566926\n",
      "Elapsed time 1.18 seconds\n",
      "Epoch: [5/8], Step: [31/31], Training Acc: 78.7792943600309\n",
      "Epoch: [5/8], Step: [31/31], Validation Acc: 55.610236220472444\n",
      "Elapsed time 1.18 seconds\n",
      "Epoch: [6/8], Step: [31/31], Training Acc: 85.03734226113829\n",
      "Epoch: [6/8], Step: [31/31], Validation Acc: 54.724409448818896\n",
      "Elapsed time 1.19 seconds\n",
      "Epoch: [7/8], Step: [31/31], Training Acc: 89.02910121040432\n",
      "Epoch: [7/8], Step: [31/31], Validation Acc: 54.03543307086614\n",
      "Elapsed time 1.19 seconds\n",
      "Epoch: [8/8], Step: [31/31], Training Acc: 93.32989956219419\n",
      "Epoch: [8/8], Step: [31/31], Validation Acc: 52.55905511811024\n",
      "Elapsed time 1.18 seconds\n",
      "[(1, 30, 58.588720061807884, 53.1496062992126, 200, False), (2, 30, 63.45609065155807, 55.610236220472444, 200, False), (3, 30, 69.12181303116148, 54.82283464566929, 200, False), (4, 30, 73.96343033736801, 54.232283464566926, 200, False), (5, 30, 78.7792943600309, 55.610236220472444, 200, False), (6, 30, 85.03734226113829, 54.724409448818896, 200, False), (7, 30, 89.02910121040432, 54.03543307086614, 200, False), (8, 30, 93.32989956219419, 52.55905511811024, 200, False)]\n",
      "\n",
      "GENRE: telephone\n",
      "\n",
      "Running the model\n",
      "Epoch: [1/8], Step: [31/34], Training Acc: 54.54332552693209\n",
      "Epoch: [1/8], Step: [31/34], Validation Acc: 55.223880597014926\n",
      "Elapsed time 1.26 seconds\n",
      "Epoch: [2/8], Step: [31/34], Training Acc: 59.64871194379391\n",
      "Epoch: [2/8], Step: [31/34], Validation Acc: 57.114427860696516\n",
      "Elapsed time 1.32 seconds\n",
      "Epoch: [3/8], Step: [31/34], Training Acc: 64.33255269320843\n",
      "Epoch: [3/8], Step: [31/34], Validation Acc: 56.318407960199\n",
      "Elapsed time 1.31 seconds\n",
      "Epoch: [4/8], Step: [31/34], Training Acc: 68.5480093676815\n",
      "Epoch: [4/8], Step: [31/34], Validation Acc: 57.51243781094527\n",
      "Elapsed time 1.3 seconds\n",
      "Epoch: [5/8], Step: [31/34], Training Acc: 73.34894613583138\n",
      "Epoch: [5/8], Step: [31/34], Validation Acc: 57.711442786069654\n",
      "Elapsed time 1.3 seconds\n",
      "Epoch: [6/8], Step: [31/34], Training Acc: 79.46135831381733\n",
      "Epoch: [6/8], Step: [31/34], Validation Acc: 56.71641791044776\n",
      "Elapsed time 1.3 seconds\n",
      "Epoch: [7/8], Step: [31/34], Training Acc: 84.30913348946136\n",
      "Epoch: [7/8], Step: [31/34], Validation Acc: 56.318407960199\n",
      "Elapsed time 1.29 seconds\n",
      "Epoch: [8/8], Step: [31/34], Training Acc: 88.57142857142857\n",
      "Epoch: [8/8], Step: [31/34], Validation Acc: 55.62189054726368\n",
      "Elapsed time 1.29 seconds\n",
      "[(1, 30, 54.54332552693209, 55.223880597014926, 200, False), (2, 30, 59.64871194379391, 57.114427860696516, 200, False), (3, 30, 64.33255269320843, 56.318407960199, 200, False), (4, 30, 68.5480093676815, 57.51243781094527, 200, False), (5, 30, 73.34894613583138, 57.711442786069654, 200, False), (6, 30, 79.46135831381733, 56.71641791044776, 200, False), (7, 30, 84.30913348946136, 56.318407960199, 200, False), (8, 30, 88.57142857142857, 55.62189054726368, 200, False)]\n",
      "\n",
      "GENRE: travel\n",
      "\n",
      "Running the model\n",
      "Epoch: [1/8], Step: [31/32], Training Acc: 55.006273525721454\n",
      "Epoch: [1/8], Step: [31/32], Validation Acc: 49.69450101832994\n",
      "Elapsed time 1.19 seconds\n",
      "Epoch: [2/8], Step: [31/32], Training Acc: 59.72396486825596\n",
      "Epoch: [2/8], Step: [31/32], Validation Acc: 51.12016293279022\n",
      "Elapsed time 1.21 seconds\n",
      "Epoch: [3/8], Step: [31/32], Training Acc: 66.42409033877038\n",
      "Epoch: [3/8], Step: [31/32], Validation Acc: 52.13849287169043\n",
      "Elapsed time 1.2 seconds\n",
      "Epoch: [4/8], Step: [31/32], Training Acc: 72.32120451693852\n",
      "Epoch: [4/8], Step: [31/32], Validation Acc: 52.240325865580445\n",
      "Elapsed time 1.23 seconds\n",
      "Epoch: [5/8], Step: [31/32], Training Acc: 78.67001254705144\n",
      "Epoch: [5/8], Step: [31/32], Validation Acc: 53.15682281059063\n",
      "Elapsed time 1.2 seconds\n",
      "Epoch: [6/8], Step: [31/32], Training Acc: 84.71769134253451\n",
      "Epoch: [6/8], Step: [31/32], Validation Acc: 52.64765784114053\n",
      "Elapsed time 1.2 seconds\n",
      "Epoch: [7/8], Step: [31/32], Training Acc: 88.45671267252196\n",
      "Epoch: [7/8], Step: [31/32], Validation Acc: 50.91649694501018\n",
      "Elapsed time 1.2 seconds\n",
      "Epoch: [8/8], Step: [31/32], Training Acc: 93.62609786700125\n",
      "Epoch: [8/8], Step: [31/32], Validation Acc: 51.73116089613035\n",
      "Elapsed time 1.2 seconds\n",
      "[(1, 30, 55.006273525721454, 49.69450101832994, 200, False), (2, 30, 59.72396486825596, 51.12016293279022, 200, False), (3, 30, 66.42409033877038, 52.13849287169043, 200, False), (4, 30, 72.32120451693852, 52.240325865580445, 200, False), (5, 30, 78.67001254705144, 53.15682281059063, 200, False), (6, 30, 84.71769134253451, 52.64765784114053, 200, False), (7, 30, 88.45671267252196, 50.91649694501018, 200, False), (8, 30, 93.62609786700125, 51.73116089613035, 200, False)]\n"
     ]
    }
   ],
   "source": [
    "def fine_tune_mnli(mdb):\n",
    "    train_mnli_vectors, val_vectors = mdb.get_mnli_indexed_text_vectors()\n",
    "#     genres = list(set([x[3] for x in val_vectors]))\n",
    "    val_genres = {}\n",
    "    for premise, hypothesis, label, genre in val_vectors:\n",
    "        if genre not in val_genres:\n",
    "            val_genres[genre] = [(premise, hypothesis, label)]\n",
    "        else:\n",
    "            val_genres[genre] = val_genres[genre] + [(premise, hypothesis, label)]\n",
    "            \n",
    "    train_genres = {}\n",
    "    for premise, hypothesis, label, genre in train_mnli_vectors:\n",
    "        if genre not in train_genres:\n",
    "            train_genres[genre] = [(premise, hypothesis, label)]\n",
    "        else:\n",
    "            train_genres[genre] = train_genres[genre] + [(premise, hypothesis, label)]\n",
    "    \n",
    "    for genre in val_genres.keys():\n",
    "        print()\n",
    "        print(\"GENRE: \" + genre)\n",
    "\n",
    "        BATCH_SIZE = 128\n",
    "#         print([x[0] for x in train_genres[genre]][0:5])\n",
    "#         print(len([x[0] for x in train_genres[genre]]))\n",
    "#         print()\n",
    "#         print(len([x[1] for x in train_genres[genre]]))\n",
    "#         print()\n",
    "#         print([x[2] for x in train_genres[genre]][0:5])\n",
    "#         print(len([x[2] for x in train_genres[genre]]))\n",
    "#         raise\n",
    "        # print(training_vectors[0].size)\n",
    "        # print(training_vectors[2])\n",
    "        train_dataset2 = SnliDataset([x[0] for x in train_genres[genre]], \n",
    "                              [x[1] for x in train_genres[genre]], \n",
    "                              [x[2] for x in train_genres[genre]])\n",
    "        train_loader2 = torch.utils.data.DataLoader(dataset=train_dataset2, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=snli_collate_func,\n",
    "                                                   shuffle=True)\n",
    "        val_dataset2 = SnliDataset([x[0] for x in val_genres[genre]], \n",
    "                              [x[1] for x in val_genres[genre]], \n",
    "                              [x[2] for x in val_genres[genre]])\n",
    "        val_loader2 = torch.utils.data.DataLoader(dataset=val_dataset2, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=snli_collate_func,\n",
    "                                                   shuffle=True)\n",
    "        best_model_cnn = torch.load(\"cnn_best_model_200_False.pt\")\n",
    "        accuracies_cnn = []\n",
    "        cnn_learning_rate = 3e-4\n",
    "        num_epochs = 8\n",
    "        run_model(mdb, best_model_cnn, accuracies_cnn, cnn_learning_rate, num_epochs, train_loader2, val_loader2, 'cnn_mnli')\n",
    "        pkl.dump(accuracies_cnn, open(\"cnn_accuracies_{}\".format(genre), \"wb\"))\n",
    "        print(accuracies_cnn)\n",
    "#         raise\n",
    "        \n",
    "fine_tune_mnli(mdb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
