{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary: you can call line.split() to immediately get the tokenized version of \n",
    "# the sentences. Normally you would have to run the text through a proper tokenizer.\n",
    "\"\"\"\n",
    "You should create a subclass for Dataset, but not for Dataloader. \n",
    "You should probably also define a custom collate_fn, \n",
    "but you should also only need to do this once.\n",
    "You will instantiate separate Datasets: e.g. train_dataset and val_dataset. \n",
    "Then you should create a Dataloader around each one: train_dataloader and val_dataloader.\n",
    "For your max length, that should be determined based on your training set \n",
    "(you should pretend you do not have access to your validation set when choosing \n",
    "such \"hyperparameters\", to avoid biasing your results.\n",
    "\n",
    "sent1_vector = rnn(sent1)\n",
    "sent2_vector = rnn(sent2)\n",
    "combined_vector = torch.cat([sent1_vector, sent2_vector], dim=1)\n",
    "\n",
    "nn.Sequential(nn.Linear(x, hidden_size), \n",
    "              nn.ReLU(inplace=True), \n",
    "              nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "You are required to concatenate the representations, not the sentences. \n",
    "The idea is that you will encode each of your sentences through an \n",
    "encoder (CNN or RNN) and extract a fixed-length vector representation \n",
    "of the sentences. You will then concatenate the two representations and \n",
    "feed that through a fully-connected layer.\n",
    "\n",
    "Transfer the string into word vectors using fast text, process \n",
    "them with data loader, feed them separately into the encoder, \n",
    "get the output representation, concat them and then fed that into a \n",
    "fully-connected layer for classification.\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelDatasetBuilder(object):\n",
    "    \"\"\"\n",
    "    Use this class to build the datasets for model consumption\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, vocab_size=10000):\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.TKN_OFFSET = 2\n",
    "        self.EMB_SIZE = 300\n",
    "        self.vocab_size = vocab_size\n",
    "        self.id2token = []\n",
    "        self.token2id = None\n",
    "        self.fasttext_emb_map = {}\n",
    "        pass\n",
    "    \n",
    "    def load_fasttext_vectors_into_vocabulary(self, fname):\n",
    "        self.id2token = [None] * (self.vocab_size + self.TKN_OFFSET)\n",
    "        with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n",
    "            n, d = map(int, fin.readline().split())\n",
    "            for i, line in enumerate(fin):\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                self.id2token[i + self.TKN_OFFSET] = tokens[0]\n",
    "                self.fasttext_emb_map[tokens[0]] = list(map(float, tokens[1:]))\n",
    "                if i>self.vocab_size - self.TKN_OFFSET:\n",
    "                    break\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "        self.id2token[self.PAD_IDX] = '<pad>'\n",
    "        self.id2token[self.UNK_IDX] = '<unk>'\n",
    "        self.token2id = dict(zip(self.id2token, range(0,len(self.id2token)))) \n",
    "        self.token2id['<pad>'] = self.PAD_IDX \n",
    "        self.token2id['<unk>'] = self.UNK_IDX\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "    \n",
    "    def get_indexed_data(self, data_path, max_data):\n",
    "        label_mapper = {\n",
    "            \"contradiction\\n\": 0,\n",
    "            \"entailment\\n\": 1,\n",
    "            \"neutral\\n\": 2\n",
    "        }\n",
    "        lines = []\n",
    "        with open(data_path, 'r', newline='\\n') as f:\n",
    "            next(f)\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                parts = line.split('\\t')\n",
    "                premise_tokens =  parts[0].split()\n",
    "                premise_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in premise_tokens]\n",
    "                hypothesis_tokens =  parts[1].split()\n",
    "                hypothesis_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in hypothesis_tokens]\n",
    "                label =  label_mapper[parts[2]]\n",
    "                lines.append( (premise_indices, hypothesis_indices, label) )\n",
    "                if i > max_data:\n",
    "                    break\n",
    "                i += 1\n",
    "        print(\"All of the lines!!!\")\n",
    "        print(len(lines))\n",
    "        return lines\n",
    "    \n",
    "    def get_embedding_vector(self):\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        pre_trained_embeddings = []\n",
    "        for i in range(0, self.vocab_size):\n",
    "            token = self.id2token[i]\n",
    "            pre_trained_embeddings.append(self.fasttext_emb_map[token])\n",
    "        weight = torch.FloatTensor(pre_trained_embeddings)\n",
    "        embedding = nn.Embedding.from_pretrained(weight, freeze=True)\n",
    "        return embedding\n",
    "    \n",
    "    def get_indexed_text_vectors(self, max_data=float(\"inf\")):\n",
    "        \"\"\"\n",
    "        Gets the torch.utils.data.Dataset preproccessed version\n",
    "        \"\"\"\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        training_vectors = self.get_indexed_data('hw2_data/snli_train.tsv', max_data)\n",
    "        val_vectors = self.get_indexed_data('hw2_data/snli_val.tsv', max_data)\n",
    "        return training_vectors, val_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "class SnliDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, premises, hypotheses, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.premises = premises\n",
    "        self.hypotheses = hypotheses\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.premises) == len(self.target_list))\n",
    "        assert (len(self.hypotheses) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        premise_idx = self.premises[index][:MAX_SENTENCE_LENGTH]\n",
    "        hypothesis_idx = self.hypotheses[index][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[index]\n",
    "        return [premise_idx, hypothesis_idx, len(premise_idx), len(hypothesis_idx), label]\n",
    "    \n",
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    premise_list = []\n",
    "    hypothesis_list = []\n",
    "    label_list = []\n",
    "    premise_length_list = []\n",
    "    hypothesis_length_list = []\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    for datum in sorted_batch:\n",
    "        label_list.append(datum[4])\n",
    "        premise_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "        hypothesis_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "    # padding\n",
    "    for datum in sorted_batch:\n",
    "        padded_premise_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        premise_list.append(padded_premise_vec)\n",
    "        padded_hypothesis_vec = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        hypothesis_list.append(padded_hypothesis_vec)\n",
    "\n",
    "    collation = [torch.LongTensor(premise_list), \n",
    "                 torch.LongTensor(hypothesis_list), \n",
    "                 torch.LongTensor(hypothesis_length_list), \n",
    "                 torch.LongTensor(label_list)]\n",
    "    return collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of questions\n",
    "\"\"\"\n",
    "- Do I need to instantiate separate versions of the model for each premise and hypothesis\n",
    "both linear and RNN\n",
    "- Do I have to instantiate in the init function or can I do it in the forward for things\n",
    "like the nn.Sequential operator\n",
    "- How do I parallelize it?\n",
    "- How does the conv net work?\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size        \n",
    "        super(GRU, self).__init__()\n",
    "        print(\"GRU Settings...\")\n",
    "        print(\"embeddings: \" + str(embeddings.embedding_dim))\n",
    "        print(\"hidden size: \" + str(hidden_size))\n",
    "        print(\"num layers: \" + str(num_layers))\n",
    "        print(\"num classes: \" + str(num_classes))\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = embeddings\n",
    "        self.rnn_premise = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.rnn_hypothesis = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.linear_premise = nn.Linear(hidden_size, num_classes)\n",
    "        self.linear_hyp = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.linear_fully_connected = nn.Linear(6, 32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(32, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = premises.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Run encoder for the premise\n",
    "        # get embedding of tokens\n",
    "        embed_premises = self.embedding(premises)\n",
    "        # pack padded sequence\n",
    "        embed_premises = torch.nn.utils.rnn.pack_padded_sequence(embed_premises, \n",
    "                                                                 lengths.numpy(), \n",
    "                                                                 batch_first=True)\n",
    "        # fprop though RNN\n",
    "        rnn_out_prem, self.hidden = self.rnn_premise(embed_premises, self.hidden)\n",
    "        # undo packing\n",
    "        rnn_out_prem, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_prem, \n",
    "                                                                 batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out_prem = torch.sum(rnn_out_prem, dim=1)\n",
    "        logits_prem = self.linear_premise(rnn_out_prem)\n",
    "\n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "#         embed_hyp = torch.nn.utils.rnn.pack_padded_sequence(embed_hyp, \n",
    "#                                                                  lengths.numpy(), \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp, self.hidden = self.rnn_hypothesis(embed_hyp, self.hidden)\n",
    "#         rnn_out_hyp, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_hyp, \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp = torch.sum(rnn_out_hyp, dim=1)\n",
    "        logits_hyp = self.linear_hyp(rnn_out_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = embeddings\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(embeddings.embedding_dim, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.linear_fully_connected = nn.Linear(6, 32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        batch_size, seq_len = premises.size()\n",
    "        \n",
    "        # Run encoder for the premises\n",
    "        embed_premises = self.embedding(premises)\n",
    "        hidden_prem = self.conv1(embed_premises.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "\n",
    "        hidden_prem = self.conv2(hidden_prem.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "\n",
    "        hidden_prem = torch.sum(hidden_prem, dim=1)\n",
    "        logits_prem = self.linear(hidden_prem)\n",
    "        \n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "        hidden_hyp = self.conv1(embed_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "\n",
    "        hidden_hyp = self.conv2(hidden_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "\n",
    "        hidden_hyp = torch.sum(hidden_hyp, dim=1)\n",
    "        logits_hyp = self.linear(hidden_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def run_model(mdb, model):\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 10 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (premises, hypotheses, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(premises, hypotheses, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "#             print(loss)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 5 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc))\n",
    "#                 val_acc = test_model(val_loader, model)\n",
    "#                 print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                            epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdb = ModelDatasetBuilder('hw2_data', vocab_size=50000)\n",
    "mdb.load_fasttext_vectors_into_vocabulary('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50000, 300)\n",
      "All of the lines!!!\n",
      "1002\n",
      "All of the lines!!!\n",
      "1000\n",
      "[[106, 802, 1830, 8, 9, 6265, 7167, 4388, 17, 9, 12229, 5335, 9, 563, 6, 358, 4], [106, 994, 15, 9766, 131, 3, 347, 424, 7, 86, 15, 3356, 17, 9, 2031, 1615, 19, 9, 2607, 17, 21, 4], [17790, 3, 1938, 2, 32, 84, 138, 9, 542, 402, 4], [9, 347, 8, 909, 11907, 5, 9, 884, 7167, 15, 1, 17, 3, 5656], [3543, 884, 3135, 441, 549, 17, 4808, 1826, 5472, 4]]\n",
      "[2, 1, 2, 0, 0]\n",
      "Epoch: [1/10], Step: [6/32], Training Acc: 34.231536926147704\n",
      "Epoch: [1/10], Step: [11/32], Training Acc: 35.92814371257485\n",
      "Epoch: [1/10], Step: [16/32], Training Acc: 33.43313373253493\n",
      "Epoch: [1/10], Step: [21/32], Training Acc: 33.13373253493014\n",
      "Epoch: [1/10], Step: [26/32], Training Acc: 33.23353293413174\n",
      "Epoch: [1/10], Step: [31/32], Training Acc: 34.4311377245509\n",
      "Epoch: [2/10], Step: [6/32], Training Acc: 36.02794411177645\n",
      "Epoch: [2/10], Step: [11/32], Training Acc: 37.724550898203596\n",
      "Epoch: [2/10], Step: [16/32], Training Acc: 41.516966067864274\n",
      "Epoch: [2/10], Step: [21/32], Training Acc: 43.91217564870259\n",
      "Epoch: [2/10], Step: [26/32], Training Acc: 40.818363273453095\n",
      "Epoch: [2/10], Step: [31/32], Training Acc: 44.81037924151697\n",
      "Epoch: [3/10], Step: [6/32], Training Acc: 45.30938123752495\n",
      "Epoch: [3/10], Step: [11/32], Training Acc: 45.808383233532936\n",
      "Epoch: [3/10], Step: [16/32], Training Acc: 48.003992015968066\n",
      "Epoch: [3/10], Step: [21/32], Training Acc: 45.908183632734534\n",
      "Epoch: [3/10], Step: [26/32], Training Acc: 49.40119760479042\n",
      "Epoch: [3/10], Step: [31/32], Training Acc: 46.9061876247505\n",
      "Epoch: [4/10], Step: [6/32], Training Acc: 44.510978043912175\n",
      "Epoch: [4/10], Step: [11/32], Training Acc: 47.80439121756487\n",
      "Epoch: [4/10], Step: [16/32], Training Acc: 51.796407185628745\n",
      "Epoch: [4/10], Step: [21/32], Training Acc: 49.70059880239521\n",
      "Epoch: [4/10], Step: [26/32], Training Acc: 51.39720558882235\n",
      "Epoch: [4/10], Step: [31/32], Training Acc: 52.69461077844311\n",
      "Epoch: [5/10], Step: [6/32], Training Acc: 49.600798403193615\n",
      "Epoch: [5/10], Step: [11/32], Training Acc: 49.10179640718563\n",
      "Epoch: [5/10], Step: [16/32], Training Acc: 50.0\n",
      "Epoch: [5/10], Step: [21/32], Training Acc: 51.796407185628745\n",
      "Epoch: [5/10], Step: [26/32], Training Acc: 53.992015968063875\n",
      "Epoch: [5/10], Step: [31/32], Training Acc: 53.59281437125748\n",
      "Epoch: [6/10], Step: [6/32], Training Acc: 52.9940119760479\n",
      "Epoch: [6/10], Step: [11/32], Training Acc: 54.69061876247505\n",
      "Epoch: [6/10], Step: [16/32], Training Acc: 56.786427145708586\n",
      "Epoch: [6/10], Step: [21/32], Training Acc: 56.287425149700596\n",
      "Epoch: [6/10], Step: [26/32], Training Acc: 57.08582834331337\n",
      "Epoch: [6/10], Step: [31/32], Training Acc: 55.88822355289421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-433483033796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             num_classes=3)\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-a922e2a3bb06>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(mdb, model)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ds1011/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ds1011/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = mdb.get_embedding_vector()\n",
    "print(embeddings)\n",
    "training_vectors, val_vectors = mdb.get_indexed_text_vectors(max_data=1000)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "print([x[0] for x in training_vectors][0:5])\n",
    "print([x[2] for x in training_vectors][0:5])\n",
    "# print(training_vectors[0].size)\n",
    "# print(training_vectors[2])\n",
    "train_dataset = SnliDataset([x[0] for x in training_vectors], \n",
    "                            [x[1] for x in training_vectors], \n",
    "                            [x[2] for x in training_vectors])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = SnliDataset([x[0] for x in val_vectors], \n",
    "                            [x[1] for x in val_vectors], \n",
    "                            [x[2] for x in val_vectors])\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# model = GRU(embeddings=embeddings, \n",
    "#             hidden_size=200, \n",
    "#             num_layers=2, \n",
    "#             num_classes=3)\n",
    "\n",
    "model = CNN(embeddings=embeddings, \n",
    "            hidden_size=200, \n",
    "            num_layers=2, \n",
    "            num_classes=3)\n",
    "\n",
    "run_model(mdb, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def \n",
    "\n",
    "mdb = ModelDatasetBuilder('hw2_data', vocab_size=50000)\n",
    "mdb.load_fasttext_vectors_into_vocabulary('wiki-news-300d-1M.vec')\n",
    "embeddings = mdb.get_embedding_vector()\n",
    "print(embeddings)\n",
    "training_vectors, val_vectors = mdb.get_indexed_text_vectors()\n",
    "print(training_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
