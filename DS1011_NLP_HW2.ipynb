{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-1011 NLP HW 2 Code Base\n",
    "\n",
    "## Part 1. Generating Text Vectors and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# dtype = torch.FloatTensor\n",
    "dtype_float = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "dtype_long = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary: you can call line.split() to immediately get the tokenized version of \n",
    "# the sentences. Normally you would have to run the text through a proper tokenizer.\n",
    "\"\"\"\n",
    "You should create a subclass for Dataset, but not for Dataloader. \n",
    "You should probably also define a custom collate_fn, \n",
    "but you should also only need to do this once.\n",
    "You will instantiate separate Datasets: e.g. train_dataset and val_dataset. \n",
    "Then you should create a Dataloader around each one: train_dataloader and val_dataloader.\n",
    "For your max length, that should be determined based on your training set \n",
    "(you should pretend you do not have access to your validation set when choosing \n",
    "such \"hyperparameters\", to avoid biasing your results.\n",
    "\n",
    "sent1_vector = rnn(sent1)\n",
    "sent2_vector = rnn(sent2)\n",
    "combined_vector = torch.cat([sent1_vector, sent2_vector], dim=1)\n",
    "\n",
    "nn.Sequential(nn.Linear(x, hidden_size), \n",
    "              nn.ReLU(inplace=True), \n",
    "              nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "You are required to concatenate the representations, not the sentences. \n",
    "The idea is that you will encode each of your sentences through an \n",
    "encoder (CNN or RNN) and extract a fixed-length vector representation \n",
    "of the sentences. You will then concatenate the two representations and \n",
    "feed that through a fully-connected layer.\n",
    "\n",
    "Transfer the string into word vectors using fast text, process \n",
    "them with data loader, feed them separately into the encoder, \n",
    "get the output representation, concat them and then fed that into a \n",
    "fully-connected layer for classification.\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelDatasetBuilder(object):\n",
    "    \"\"\"\n",
    "    Use this class to build the datasets for model consumption\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, vocab_size=10000):\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.TKN_OFFSET = 2\n",
    "        self.EMB_SIZE = 300\n",
    "        self.vocab_size = vocab_size\n",
    "        self.id2token = []\n",
    "        self.token2id = None\n",
    "        self.fasttext_emb_map = {}\n",
    "        pass\n",
    "    \n",
    "    def load_fasttext_vectors_into_vocabulary(self, fname):\n",
    "        self.id2token = [None] * (self.vocab_size + self.TKN_OFFSET)\n",
    "        with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n",
    "            n, d = map(int, fin.readline().split())\n",
    "            for i, line in enumerate(fin):\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                self.id2token[i + self.TKN_OFFSET] = tokens[0]\n",
    "                self.fasttext_emb_map[tokens[0]] = list(map(float, tokens[1:]))\n",
    "                if i>self.vocab_size - self.TKN_OFFSET:\n",
    "                    break\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "        self.id2token[self.PAD_IDX] = '<pad>'\n",
    "        self.id2token[self.UNK_IDX] = '<unk>'\n",
    "        self.token2id = dict(zip(self.id2token, range(0,len(self.id2token)))) \n",
    "        self.token2id['<pad>'] = self.PAD_IDX \n",
    "        self.token2id['<unk>'] = self.UNK_IDX\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "    \n",
    "    def get_indexed_data(self, data_path, max_data, mnli=False):\n",
    "        label_mapper = {\n",
    "            \"contradiction\\n\": 0,\n",
    "            \"contradiction\": 0,\n",
    "            \"entailment\\n\": 1,\n",
    "            \"entailment\": 1,\n",
    "            \"neutral\\n\": 2,\n",
    "            \"neutral\": 2\n",
    "        }\n",
    "        lines = []\n",
    "        with open(data_path, 'r', newline='\\n') as f:\n",
    "            next(f)\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                parts = line.split('\\t')\n",
    "                premise_tokens =  parts[0].split()\n",
    "                premise_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in premise_tokens]\n",
    "                hypothesis_tokens =  parts[1].split()\n",
    "                hypothesis_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in hypothesis_tokens]\n",
    "                label =  label_mapper[parts[2]]\n",
    "                if mnli:\n",
    "                    cat = parts[3]\n",
    "                    row = premise_indices, hypothesis_indices, label, cat\n",
    "                else:\n",
    "                    row = premise_indices, hypothesis_indices, label\n",
    "                lines.append( row )\n",
    "                if i > max_data:\n",
    "                    break\n",
    "                i += 1\n",
    "        print(\"All of the lines!!!\")\n",
    "        print(len(lines))\n",
    "        return lines\n",
    "    \n",
    "    def get_embedding_vector(self):\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        pre_trained_embeddings = []\n",
    "        for i in range(0, self.vocab_size):\n",
    "            token = self.id2token[i]\n",
    "            pre_trained_embeddings.append(self.fasttext_emb_map[token])\n",
    "#         weight = torch.FloatTensor(pre_trained_embeddings)\n",
    "        weight = torch.FloatTensor(pre_trained_embeddings).type(dtype_float).cuda()\n",
    "#         weight = weight.long()\n",
    "        embedding = nn.Embedding.from_pretrained(weight, freeze=True).cuda()\n",
    "        return embedding\n",
    "    \n",
    "    def get_indexed_text_vectors(self, max_data=float(\"inf\")):\n",
    "        \"\"\"\n",
    "        Gets the torch.utils.data.Dataset preproccessed version\n",
    "        \"\"\"\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        training_vectors = self.get_indexed_data('hw2_data/snli_train.tsv', max_data)\n",
    "        val_vectors = self.get_indexed_data('hw2_data/snli_val.tsv', max_data)\n",
    "        return training_vectors, val_vectors\n",
    "    \n",
    "    def get_mnli_indexed_text_vectors(self, max_data=float(\"inf\")):\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        mnli_vectors = self.get_indexed_data('hw2_data/mnli_val.tsv', max_data, mnli=True)\n",
    "        return mnli_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "class SnliDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, premises, hypotheses, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.premises = premises\n",
    "        self.hypotheses = hypotheses\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.premises) == len(self.target_list))\n",
    "        assert (len(self.hypotheses) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        premise_idx = self.premises[index][:MAX_SENTENCE_LENGTH]\n",
    "        hypothesis_idx = self.hypotheses[index][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[index]\n",
    "        return [premise_idx, hypothesis_idx, len(premise_idx), len(hypothesis_idx), label]\n",
    "    \n",
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    premise_list = []\n",
    "    hypothesis_list = []\n",
    "    label_list = []\n",
    "    premise_length_list = []\n",
    "    hypothesis_length_list = []\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    for datum in sorted_batch:\n",
    "        label_list.append(datum[4])\n",
    "        premise_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "        hypothesis_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "    # padding\n",
    "    for datum in sorted_batch:\n",
    "        padded_premise_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        premise_list.append(padded_premise_vec)\n",
    "        padded_hypothesis_vec = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        hypothesis_list.append(padded_hypothesis_vec)\n",
    "\n",
    "    collation = [torch.FloatTensor(premise_list).type(dtype_long), \n",
    "                 torch.FloatTensor(hypothesis_list).type(dtype_long), \n",
    "                 torch.FloatTensor(hypothesis_length_list).type(dtype_long), \n",
    "                 torch.FloatTensor(label_list).type(dtype_long)]\n",
    "    return collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of questions\n",
    "\"\"\"\n",
    "- Do I need to instantiate separate versions of the model for each premise and hypothesis\n",
    "both linear and RNN\n",
    "- Do I have to instantiate in the init function or can I do it in the forward for things\n",
    "like the nn.Sequential operator\n",
    "- How do I parallelize it?\n",
    "- How does the conv net work?\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes, element_wise=False):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size        \n",
    "        super(GRU, self).__init__()\n",
    "        print(\"GRU Settings...\")\n",
    "        print(\"embeddings: \" + str(embeddings.embedding_dim))\n",
    "        print(\"hidden size: \" + str(hidden_size))\n",
    "        print(\"num layers: \" + str(num_layers))\n",
    "        print(\"num classes: \" + str(num_classes))\n",
    "        self.num_layers, self.hidden_size, self.element_wise = num_layers, hidden_size, element_wise\n",
    "        self.embedding = embeddings.cuda()\n",
    "        self.rnn_premise = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.rnn_hypothesis = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.linear_premise = nn.Linear(hidden_size, num_classes)\n",
    "        self.linear_hyp = nn.Linear(hidden_size, num_classes)\n",
    "        if element_wise:\n",
    "            fully_connected_dim = 128\n",
    "            self.linear_fully_connected = nn.Linear(3, fully_connected_dim)\n",
    "        else:\n",
    "            fully_connected_dim = 32\n",
    "            self.linear_fully_connected = nn.Linear(6, fully_connected_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(fully_connected_dim, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = premises.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Run encoder for the premise\n",
    "        # get embedding of tokens\n",
    "        embed_premises = self.embedding(premises)\n",
    "        # pack padded sequence\n",
    "        embed_premises = torch.nn.utils.rnn.pack_padded_sequence(embed_premises, \n",
    "                                                                 lengths, \n",
    "                                                                 batch_first=True)\n",
    "        # fprop though RNN\n",
    "        rnn_out_prem, self.hidden = self.rnn_premise(embed_premises, self.hidden)\n",
    "        # undo packing\n",
    "        rnn_out_prem, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_prem, \n",
    "                                                                 batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out_prem = torch.sum(rnn_out_prem, dim=1)\n",
    "        logits_prem = self.linear_premise(rnn_out_prem)\n",
    "\n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "#         embed_hyp = torch.nn.utils.rnn.pack_padded_sequence(embed_hyp, \n",
    "#                                                                  lengths.numpy(), \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp, self.hidden = self.rnn_hypothesis(embed_hyp, self.hidden)\n",
    "#         rnn_out_hyp, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_hyp, \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp = torch.sum(rnn_out_hyp, dim=1)\n",
    "        logits_hyp = self.linear_hyp(rnn_out_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        if self.element_wise:\n",
    "#             print(logits_prem.size())\n",
    "#             print(logits_hyp.size())\n",
    "            combined_sentences = torch.mul(logits_prem, logits_hyp)\n",
    "#             print(combined_sentences.size())\n",
    "        else:\n",
    "            combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes, kernel_size=3, element_wise=False):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = embeddings.cuda()\n",
    "        self.element_wise = element_wise\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(embeddings.embedding_dim, hidden_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=1)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool1d(kernel_size, stride=2)\n",
    "        self.linear = nn.Linear( int(np.ceil(hidden_size / 2.0) - 1), 128)\n",
    "        #128 x 256\n",
    "        if element_wise:\n",
    "            fully_connected_dim = 128\n",
    "            self.linear_fully_connected = nn.Linear(3, fully_connected_dim)\n",
    "        else:\n",
    "            fully_connected_dim = 256\n",
    "            self.linear_fully_connected = nn.Linear(6, fully_connected_dim)\n",
    "        self.linear_fully_connected = nn.Linear(fully_connected_dim, 128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        premises, hypotheses, lengths = premises.cuda(), hypotheses.cuda(), lengths.cuda()\n",
    "        \n",
    "        batch_size, seq_len = premises.size()\n",
    "        \n",
    "        # Run encoder for the premises\n",
    "#         print(\"DEGUB-----\")\n",
    "#         print(premises)\n",
    "#         print(premises.double())\n",
    "#         print(premises.int())\n",
    "#         print(premises.type(torch.cuda.LongTensor))\n",
    "        embed_premises = self.embedding(premises)\n",
    "        hidden_prem = self.conv1(embed_premises.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "\n",
    "        hidden_prem = self.conv2(hidden_prem.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "        \n",
    "        pooled_prem = self.max_pool(hidden_prem)\n",
    "\n",
    "        hidden_prem = torch.sum(pooled_prem, dim=1)\n",
    "        logits_prem = self.linear(hidden_prem)\n",
    "        \n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "        hidden_hyp = self.conv1(embed_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "\n",
    "        hidden_hyp = self.conv2(hidden_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "        \n",
    "        pooled_hyp = self.max_pool(hidden_hyp)\n",
    "\n",
    "        hidden_hyp = torch.sum(pooled_hyp, dim=1)\n",
    "        logits_hyp = self.linear(hidden_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        if self.element_wise:\n",
    "            combined_sentences = torch.mul(logits_prem, logits_hyp)\n",
    "        else:\n",
    "            combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Model Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def run_model(mdb, model, accuracies, learning_rate, num_epochs, train_loader, val_loader, prefix):\n",
    "    model.cuda()\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    start = time()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (premises, hypotheses, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(premises, hypotheses, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "#             print(loss)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc))\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                end = time()\n",
    "                print(\"Elapsed time {} seconds\".format(round(end - start, 2)))\n",
    "                start = time()\n",
    "                accuracies.append( (epoch+1, i, train_acc, val_acc, model.hidden_size, model.element_wise) )\n",
    "                if val_acc > 69:\n",
    "                    pkl.dump(accuracies, open(\"{}_accuracies5_{}_{}\".format(prefix, model.hidden_size, model.element_wise), \"wb\"))\n",
    "                    torch.save(model, \n",
    "                               \"{}_best_model_{}_{}.pt\".format(prefix, model.hidden_size, model.element_wise))\n",
    "                    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdb = ModelDatasetBuilder('hw2_data', vocab_size=50000)\n",
    "mdb.load_fasttext_vectors_into_vocabulary('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50000, 300)\n",
      "All of the lines!!!\n",
      "100000\n",
      "All of the lines!!!\n",
      "1000\n",
      "[[106, 802, 1830, 8, 9, 6265, 7167, 4388, 17, 9, 12229, 5335, 9, 563, 6, 358, 4], [106, 994, 15, 9766, 131, 3, 347, 424, 7, 86, 15, 3356, 17, 9, 2031, 1615, 19, 9, 2607, 17, 21, 4], [17790, 3, 1938, 2, 32, 84, 138, 9, 542, 402, 4], [9, 347, 8, 909, 11907, 5, 9, 884, 7167, 15, 1, 17, 3, 5656], [3543, 884, 3135, 441, 549, 17, 4808, 1826, 5472, 4]]\n",
      "[2, 1, 2, 0, 0]\n",
      "hidden size 200\n",
      "element wise: False\n",
      "Epoch: [1/10], Step: [101/782], Training Acc: 47.072\n",
      "Epoch: [1/10], Step: [101/782], Validation Acc: 44.9\n",
      "Elapsed time 14.68 seconds\n",
      "Epoch: [1/10], Step: [201/782], Training Acc: 57.139\n",
      "Epoch: [1/10], Step: [201/782], Validation Acc: 57.5\n",
      "Elapsed time 14.78 seconds\n",
      "Epoch: [1/10], Step: [301/782], Training Acc: 59.263\n",
      "Epoch: [1/10], Step: [301/782], Validation Acc: 58.5\n",
      "Elapsed time 14.94 seconds\n",
      "Epoch: [1/10], Step: [401/782], Training Acc: 60.528\n",
      "Epoch: [1/10], Step: [401/782], Validation Acc: 58.3\n",
      "Elapsed time 15.02 seconds\n",
      "Epoch: [1/10], Step: [501/782], Training Acc: 61.436\n",
      "Epoch: [1/10], Step: [501/782], Validation Acc: 59.5\n",
      "Elapsed time 14.65 seconds\n",
      "Epoch: [1/10], Step: [601/782], Training Acc: 62.508\n",
      "Epoch: [1/10], Step: [601/782], Validation Acc: 60.4\n",
      "Elapsed time 14.8 seconds\n",
      "Epoch: [1/10], Step: [701/782], Training Acc: 63.145\n",
      "Epoch: [1/10], Step: [701/782], Validation Acc: 60.5\n",
      "Elapsed time 14.86 seconds\n",
      "Epoch: [2/10], Step: [101/782], Training Acc: 64.483\n",
      "Epoch: [2/10], Step: [101/782], Validation Acc: 62.0\n",
      "Elapsed time 17.04 seconds\n",
      "Epoch: [2/10], Step: [201/782], Training Acc: 64.905\n",
      "Epoch: [2/10], Step: [201/782], Validation Acc: 62.8\n",
      "Elapsed time 15.52 seconds\n",
      "Epoch: [2/10], Step: [301/782], Training Acc: 65.595\n",
      "Epoch: [2/10], Step: [301/782], Validation Acc: 61.6\n",
      "Elapsed time 14.83 seconds\n",
      "Epoch: [2/10], Step: [401/782], Training Acc: 66.025\n",
      "Epoch: [2/10], Step: [401/782], Validation Acc: 62.7\n",
      "Elapsed time 14.69 seconds\n",
      "Epoch: [2/10], Step: [501/782], Training Acc: 65.872\n",
      "Epoch: [2/10], Step: [501/782], Validation Acc: 60.0\n",
      "Elapsed time 14.82 seconds\n",
      "Epoch: [2/10], Step: [601/782], Training Acc: 66.095\n",
      "Epoch: [2/10], Step: [601/782], Validation Acc: 63.0\n",
      "Elapsed time 14.74 seconds\n",
      "Epoch: [2/10], Step: [701/782], Training Acc: 67.232\n",
      "Epoch: [2/10], Step: [701/782], Validation Acc: 65.4\n",
      "Elapsed time 14.72 seconds\n",
      "Epoch: [3/10], Step: [101/782], Training Acc: 68.479\n",
      "Epoch: [3/10], Step: [101/782], Validation Acc: 65.1\n",
      "Elapsed time 16.37 seconds\n",
      "Epoch: [3/10], Step: [201/782], Training Acc: 68.729\n",
      "Epoch: [3/10], Step: [201/782], Validation Acc: 64.5\n",
      "Elapsed time 14.7 seconds\n",
      "Epoch: [3/10], Step: [301/782], Training Acc: 69.675\n",
      "Epoch: [3/10], Step: [301/782], Validation Acc: 64.4\n",
      "Elapsed time 14.6 seconds\n",
      "Epoch: [3/10], Step: [401/782], Training Acc: 68.158\n",
      "Epoch: [3/10], Step: [401/782], Validation Acc: 65.3\n",
      "Elapsed time 14.91 seconds\n",
      "Epoch: [3/10], Step: [501/782], Training Acc: 70.251\n",
      "Epoch: [3/10], Step: [501/782], Validation Acc: 65.6\n",
      "Elapsed time 14.86 seconds\n",
      "Epoch: [3/10], Step: [601/782], Training Acc: 70.891\n",
      "Epoch: [3/10], Step: [601/782], Validation Acc: 66.6\n",
      "Elapsed time 14.8 seconds\n",
      "Epoch: [3/10], Step: [701/782], Training Acc: 71.143\n",
      "Epoch: [3/10], Step: [701/782], Validation Acc: 67.5\n",
      "Elapsed time 14.78 seconds\n",
      "Epoch: [4/10], Step: [101/782], Training Acc: 71.578\n",
      "Epoch: [4/10], Step: [101/782], Validation Acc: 66.3\n",
      "Elapsed time 16.23 seconds\n",
      "Epoch: [4/10], Step: [201/782], Training Acc: 71.216\n",
      "Epoch: [4/10], Step: [201/782], Validation Acc: 66.0\n",
      "Elapsed time 14.99 seconds\n",
      "Epoch: [4/10], Step: [301/782], Training Acc: 72.577\n",
      "Epoch: [4/10], Step: [301/782], Validation Acc: 67.5\n",
      "Elapsed time 14.74 seconds\n",
      "Epoch: [4/10], Step: [401/782], Training Acc: 72.507\n",
      "Epoch: [4/10], Step: [401/782], Validation Acc: 67.7\n",
      "Elapsed time 14.71 seconds\n",
      "Epoch: [4/10], Step: [501/782], Training Acc: 72.695\n",
      "Epoch: [4/10], Step: [501/782], Validation Acc: 67.6\n",
      "Elapsed time 14.75 seconds\n",
      "Epoch: [4/10], Step: [601/782], Training Acc: 73.256\n",
      "Epoch: [4/10], Step: [601/782], Validation Acc: 67.9\n",
      "Elapsed time 14.78 seconds\n",
      "Epoch: [4/10], Step: [701/782], Training Acc: 73.272\n",
      "Epoch: [4/10], Step: [701/782], Validation Acc: 68.5\n",
      "Elapsed time 15.02 seconds\n",
      "Epoch: [5/10], Step: [101/782], Training Acc: 74.158\n",
      "Epoch: [5/10], Step: [101/782], Validation Acc: 68.8\n",
      "Elapsed time 17.82 seconds\n",
      "Epoch: [5/10], Step: [201/782], Training Acc: 74.647\n",
      "Epoch: [5/10], Step: [201/782], Validation Acc: 69.3\n",
      "Elapsed time 14.84 seconds\n",
      "[(1, 100, 47.072, 44.9, 200, False), (1, 200, 57.139, 57.5, 200, False), (1, 300, 59.263, 58.5, 200, False), (1, 400, 60.528, 58.3, 200, False), (1, 500, 61.436, 59.5, 200, False), (1, 600, 62.508, 60.4, 200, False), (1, 700, 63.145, 60.5, 200, False), (2, 100, 64.483, 62.0, 200, False), (2, 200, 64.905, 62.8, 200, False), (2, 300, 65.595, 61.6, 200, False), (2, 400, 66.025, 62.7, 200, False), (2, 500, 65.872, 60.0, 200, False), (2, 600, 66.095, 63.0, 200, False), (2, 700, 67.232, 65.4, 200, False), (3, 100, 68.479, 65.1, 200, False), (3, 200, 68.729, 64.5, 200, False), (3, 300, 69.675, 64.4, 200, False), (3, 400, 68.158, 65.3, 200, False), (3, 500, 70.251, 65.6, 200, False), (3, 600, 70.891, 66.6, 200, False), (3, 700, 71.143, 67.5, 200, False), (4, 100, 71.578, 66.3, 200, False), (4, 200, 71.216, 66.0, 200, False), (4, 300, 72.577, 67.5, 200, False), (4, 400, 72.507, 67.7, 200, False), (4, 500, 72.695, 67.6, 200, False), (4, 600, 73.256, 67.9, 200, False), (4, 700, 73.272, 68.5, 200, False), (5, 100, 74.158, 68.8, 200, False), (5, 200, 74.647, 69.3, 200, False)]\n"
     ]
    }
   ],
   "source": [
    "def run_scenario(mbd):\n",
    "    embeddings = mdb.get_embedding_vector()\n",
    "    print(embeddings)\n",
    "    training_vectors, val_vectors = mdb.get_indexed_text_vectors(max_data=1000000)\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    print([x[0] for x in training_vectors][0:5])\n",
    "    print([x[2] for x in training_vectors][0:5])\n",
    "    # print(training_vectors[0].size)\n",
    "    # print(training_vectors[2])\n",
    "    train_dataset = SnliDataset([x[0] for x in training_vectors], \n",
    "                                [x[1] for x in training_vectors], \n",
    "                                [x[2] for x in training_vectors])\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    val_dataset = SnliDataset([x[0] for x in val_vectors], \n",
    "                                [x[1] for x in val_vectors], \n",
    "                                [x[2] for x in val_vectors])\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    num_epochs = 10\n",
    "#     hidden_layers = [25, 50, 100, 200, 400]\n",
    "#     interactions = [True, False]\n",
    "    interactions = [False]\n",
    "    hidden_layers = [200]\n",
    "    for layer in hidden_layers:\n",
    "        print(\"hidden size \"+ str(layer))\n",
    "        for elmt_wise in interactions:\n",
    "            print(\"element wise: \" + str(elmt_wise))\n",
    "#             gru_learning_rate = 1e-3\n",
    "#             model_gru = GRU(embeddings=embeddings, \n",
    "#                         hidden_size=layer, \n",
    "#                         num_layers=2, \n",
    "#                         num_classes=3,\n",
    "#                         element_wise=elmt_wise)\n",
    "#             accuracies_gru = []\n",
    "#             run_model(mdb, model_gru, accuracies_gru, gru_learning_rate, num_epochs, train_loader, val_loader, 'gru')\n",
    "#             pkl.dump(accuracies_gru, \n",
    "#                      open(\"gru_accuracies4_{}_{}_{}\".format(gru_learning_rate,layer, elmt_wise), \"wb\"))\n",
    "#             print(accuracies_gru)\n",
    "        \n",
    "            cnn_learning_rate = 3e-4\n",
    "            model_cnn = CNN(embeddings=embeddings, \n",
    "                        hidden_size=layer, \n",
    "                        num_layers=2, \n",
    "                        num_classes=3,\n",
    "                        element_wise=elmt_wise)\n",
    "            accuracies_cnn = []\n",
    "            run_model(mdb, model_cnn, accuracies_cnn, cnn_learning_rate, num_epochs, train_loader, val_loader, 'cnn')\n",
    "            pkl.dump(accuracies_cnn, open(\"cnn_accuracies4_{}_{}_{}\".format(cnn_learning_rate,layer,elmt_wise), \"wb\"))\n",
    "            print(accuracies_cnn)\n",
    "\n",
    "run_scenario(mdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden size 200\n",
      "GRU Settings...\n",
      "embeddings: 300\n",
      "hidden size: 200\n",
      "num layers: 2\n",
      "num classes: 3\n",
      "1086905\n",
      "330099\n",
      "hidden size 200\n",
      "GRU Settings...\n",
      "embeddings: 300\n",
      "hidden size: 200\n",
      "num layers: 2\n",
      "num classes: 3\n",
      "1086329\n",
      "346483\n",
      "[(1086905, 200), (330099, 200), (1086329, 200), (346483, 200)]\n"
     ]
    }
   ],
   "source": [
    "def get_num_model_params(model, layer):\n",
    "    # https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "    return pytorch_total_params, layer\n",
    "\n",
    "# hidden_layers = [25, 50, 100, 200, 400]\n",
    "hidden_layers = [200]\n",
    "params = []\n",
    "wises = [True, False]\n",
    "for layer in hidden_layers:\n",
    "    for elmt_wise in wises:\n",
    "        print(\"hidden size \"+ str(layer))\n",
    "        gru_learning_rate = 1e-3\n",
    "        model_gru = GRU(embeddings=mdb.get_embedding_vector(), \n",
    "                    hidden_size=layer, \n",
    "                    num_layers=2, \n",
    "                    num_classes=3,\n",
    "                    element_wise=elmt_wise)\n",
    "        params.append(get_num_model_params(model_gru, layer))\n",
    "        model_cnn = CNN(embeddings=mdb.get_embedding_vector(), \n",
    "                        hidden_size=layer, \n",
    "                        num_layers=2, \n",
    "                        num_classes=3,\n",
    "                       element_wise=elmt_wise)\n",
    "        params.append(get_num_model_params(model_cnn, layer))\n",
    "\n",
    "print(params) # 1086329\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_correct_and_incorrect_predictions(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "best_model = torch.load(\"cnn_best_model_200_False.pt\")\n",
    "correct, incorrect = get_correct_and_incorrect_predictions(val_loader, model)\n",
    "print(\"Correct: {}\".format(correct))\n",
    "print(\"Incorrect: {}\".format(incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Plotting (see \"DS1011_NLP_HW2_graphing\" notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this so your plots show properly\n",
    "# See rest of graph generation code in other notebook \"DS1011_NLP_HW2_graphing\"\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "def get_plot_values(accuracies):\n",
    "    training_acc = [acc[2] for acc in accuracies]\n",
    "    val_acc = [acc[3] for acc in accuracies]\n",
    "    training_acc_fmt = []\n",
    "    for i, acc in enumerate(training_acc):\n",
    "        if i % 6 == 0:\n",
    "            continue\n",
    "        elif i % 7 == 0:\n",
    "            continue\n",
    "        else:\n",
    "            training_acc_fmt.append(acc)\n",
    "    val_acc_fmt = []\n",
    "    for i, acc in enumerate(val_acc):\n",
    "        if i % 6 == 0:\n",
    "            continue\n",
    "        elif i % 7 == 0:\n",
    "            continue\n",
    "        else:\n",
    "            val_acc_fmt.append(acc)\n",
    "    return training_acc_fmt, val_acc_fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 5. Evaluating on MultiNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50000, 300)\n",
      "All of the lines!!!\n",
      "5000\n",
      "telephone\n",
      "\n",
      "1005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:56: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:69: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acc for GRU is 47.46268656716418\n",
      "The acc for CNN is 49.45273631840796\n",
      "government\n",
      "\n",
      "1016\n",
      "The acc for GRU is 45.57086614173228\n",
      "The acc for CNN is 43.503937007874015\n",
      "travel\n",
      "\n",
      "982\n",
      "The acc for GRU is 47.55600814663951\n",
      "The acc for CNN is 44.09368635437882\n",
      "fiction\n",
      "\n",
      "995\n",
      "The acc for GRU is 44.42211055276382\n",
      "The acc for CNN is 44.321608040201006\n",
      "slate\n",
      "\n",
      "1002\n",
      "The acc for GRU is 43.812375249500995\n",
      "The acc for CNN is 42.81437125748503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntorch.save(the_model, PATH)\\n\\nmodel = Classifier()\\ntorch.save(model.state_dict(), ‘./model_Q2.pth’)\\nmodel.load_state_dict(torch.load(’./model_Q2.pth’, map_location=lambda storage, loc: storage))\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_mnli(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def run_mnli_scenario(mdb):\n",
    "    embeddings = mdb.get_embedding_vector()\n",
    "    print(embeddings)\n",
    "    val_vectors = mdb.get_mnli_indexed_text_vectors()\n",
    "#     genres = list(set([x[3] for x in val_vectors]))\n",
    "    genres = {}\n",
    "    for premise, hypothesis, label, genre in val_vectors:\n",
    "        if genre not in genres:\n",
    "            genres[genre] = [(premise, hypothesis, label)]\n",
    "        else:\n",
    "            genres[genre] = genres[genre] + [(premise, hypothesis, label)]\n",
    "                \n",
    "    for genre in genres.keys():\n",
    "        print(genre)\n",
    "        print(len(genres[genre]))\n",
    "\n",
    "        BATCH_SIZE = 128\n",
    "# #         print([x[0] for x in genres[genre]][0:5])\n",
    "#         print()\n",
    "#         print([x[2] for x in genres[genre]][0:5])\n",
    "\n",
    "        # print(training_vectors[0].size)\n",
    "        # print(training_vectors[2])\n",
    "        dataset = SnliDataset([x[0] for x in genres[genre]], \n",
    "                              [x[1] for x in genres[genre]], \n",
    "                              [x[2] for x in genres[genre]])\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=snli_collate_func,\n",
    "                                                   shuffle=True)\n",
    "        model = torch.load(\"gru_best_model_200_False.pt\")\n",
    "        val_acc = test_model(val_loader, model)\n",
    "        print(\"The acc for GRU is {}\".format(val_acc))\n",
    "        model = torch.load(\"cnn_best_model_200_False.pt\")\n",
    "        val_acc = test_model(val_loader, model)\n",
    "        print(\"The acc for CNN is {}\".format(val_acc))\n",
    "\n",
    "    \n",
    "run_mnli_scenario(mdb)\n",
    "\n",
    "\"\"\"\n",
    "torch.save(the_model, PATH)\n",
    "\n",
    "model = Classifier()\n",
    "torch.save(model.state_dict(), ‘./model_Q2.pth’)\n",
    "model.load_state_dict(torch.load(’./model_Q2.pth’, map_location=lambda storage, loc: storage))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
