{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# dtype = torch.FloatTensor\n",
    "dtype_float = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "dtype_long = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary: you can call line.split() to immediately get the tokenized version of \n",
    "# the sentences. Normally you would have to run the text through a proper tokenizer.\n",
    "\"\"\"\n",
    "You should create a subclass for Dataset, but not for Dataloader. \n",
    "You should probably also define a custom collate_fn, \n",
    "but you should also only need to do this once.\n",
    "You will instantiate separate Datasets: e.g. train_dataset and val_dataset. \n",
    "Then you should create a Dataloader around each one: train_dataloader and val_dataloader.\n",
    "For your max length, that should be determined based on your training set \n",
    "(you should pretend you do not have access to your validation set when choosing \n",
    "such \"hyperparameters\", to avoid biasing your results.\n",
    "\n",
    "sent1_vector = rnn(sent1)\n",
    "sent2_vector = rnn(sent2)\n",
    "combined_vector = torch.cat([sent1_vector, sent2_vector], dim=1)\n",
    "\n",
    "nn.Sequential(nn.Linear(x, hidden_size), \n",
    "              nn.ReLU(inplace=True), \n",
    "              nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "You are required to concatenate the representations, not the sentences. \n",
    "The idea is that you will encode each of your sentences through an \n",
    "encoder (CNN or RNN) and extract a fixed-length vector representation \n",
    "of the sentences. You will then concatenate the two representations and \n",
    "feed that through a fully-connected layer.\n",
    "\n",
    "Transfer the string into word vectors using fast text, process \n",
    "them with data loader, feed them separately into the encoder, \n",
    "get the output representation, concat them and then fed that into a \n",
    "fully-connected layer for classification.\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelDatasetBuilder(object):\n",
    "    \"\"\"\n",
    "    Use this class to build the datasets for model consumption\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, vocab_size=10000):\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.TKN_OFFSET = 2\n",
    "        self.EMB_SIZE = 300\n",
    "        self.vocab_size = vocab_size\n",
    "        self.id2token = []\n",
    "        self.token2id = None\n",
    "        self.fasttext_emb_map = {}\n",
    "        pass\n",
    "    \n",
    "    def load_fasttext_vectors_into_vocabulary(self, fname):\n",
    "        self.id2token = [None] * (self.vocab_size + self.TKN_OFFSET)\n",
    "        with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n",
    "            n, d = map(int, fin.readline().split())\n",
    "            for i, line in enumerate(fin):\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                self.id2token[i + self.TKN_OFFSET] = tokens[0]\n",
    "                self.fasttext_emb_map[tokens[0]] = list(map(float, tokens[1:]))\n",
    "                if i>self.vocab_size - self.TKN_OFFSET:\n",
    "                    break\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "        self.id2token[self.PAD_IDX] = '<pad>'\n",
    "        self.id2token[self.UNK_IDX] = '<unk>'\n",
    "        self.token2id = dict(zip(self.id2token, range(0,len(self.id2token)))) \n",
    "        self.token2id['<pad>'] = self.PAD_IDX \n",
    "        self.token2id['<unk>'] = self.UNK_IDX\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "    \n",
    "    def get_indexed_data(self, data_path, max_data):\n",
    "        label_mapper = {\n",
    "            \"contradiction\\n\": 0,\n",
    "            \"entailment\\n\": 1,\n",
    "            \"neutral\\n\": 2\n",
    "        }\n",
    "        lines = []\n",
    "        with open(data_path, 'r', newline='\\n') as f:\n",
    "            next(f)\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                parts = line.split('\\t')\n",
    "                premise_tokens =  parts[0].split()\n",
    "                premise_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in premise_tokens]\n",
    "                hypothesis_tokens =  parts[1].split()\n",
    "                hypothesis_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in hypothesis_tokens]\n",
    "                label =  label_mapper[parts[2]]\n",
    "                lines.append( (premise_indices, hypothesis_indices, label) )\n",
    "                if i > max_data:\n",
    "                    break\n",
    "                i += 1\n",
    "        print(\"All of the lines!!!\")\n",
    "        print(len(lines))\n",
    "        return lines\n",
    "    \n",
    "    def get_embedding_vector(self):\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        pre_trained_embeddings = []\n",
    "        for i in range(0, self.vocab_size):\n",
    "            token = self.id2token[i]\n",
    "            pre_trained_embeddings.append(self.fasttext_emb_map[token])\n",
    "#         weight = torch.FloatTensor(pre_trained_embeddings)\n",
    "        weight = torch.FloatTensor(pre_trained_embeddings).type(dtype_float).cuda()\n",
    "#         weight = weight.long()\n",
    "        embedding = nn.Embedding.from_pretrained(weight, freeze=True).cuda()\n",
    "        return embedding\n",
    "    \n",
    "    def get_indexed_text_vectors(self, max_data=float(\"inf\")):\n",
    "        \"\"\"\n",
    "        Gets the torch.utils.data.Dataset preproccessed version\n",
    "        \"\"\"\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        training_vectors = self.get_indexed_data('hw2_data/snli_train.tsv', max_data)\n",
    "        val_vectors = self.get_indexed_data('hw2_data/snli_val.tsv', max_data)\n",
    "        return training_vectors, val_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "class SnliDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, premises, hypotheses, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.premises = premises\n",
    "        self.hypotheses = hypotheses\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.premises) == len(self.target_list))\n",
    "        assert (len(self.hypotheses) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        premise_idx = self.premises[index][:MAX_SENTENCE_LENGTH]\n",
    "        hypothesis_idx = self.hypotheses[index][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[index]\n",
    "        return [premise_idx, hypothesis_idx, len(premise_idx), len(hypothesis_idx), label]\n",
    "    \n",
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    premise_list = []\n",
    "    hypothesis_list = []\n",
    "    label_list = []\n",
    "    premise_length_list = []\n",
    "    hypothesis_length_list = []\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    for datum in sorted_batch:\n",
    "        label_list.append(datum[4])\n",
    "        premise_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "        hypothesis_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "    # padding\n",
    "    for datum in sorted_batch:\n",
    "        padded_premise_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        premise_list.append(padded_premise_vec)\n",
    "        padded_hypothesis_vec = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        hypothesis_list.append(padded_hypothesis_vec)\n",
    "\n",
    "    collation = [torch.FloatTensor(premise_list).type(dtype_long), \n",
    "                 torch.FloatTensor(hypothesis_list).type(dtype_long), \n",
    "                 torch.FloatTensor(hypothesis_length_list).type(dtype_long), \n",
    "                 torch.FloatTensor(label_list).type(dtype_long)]\n",
    "    return collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of questions\n",
    "\"\"\"\n",
    "- Do I need to instantiate separate versions of the model for each premise and hypothesis\n",
    "both linear and RNN\n",
    "- Do I have to instantiate in the init function or can I do it in the forward for things\n",
    "like the nn.Sequential operator\n",
    "- How do I parallelize it?\n",
    "- How does the conv net work?\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size        \n",
    "        super(GRU, self).__init__()\n",
    "        print(\"GRU Settings...\")\n",
    "        print(\"embeddings: \" + str(embeddings.embedding_dim))\n",
    "        print(\"hidden size: \" + str(hidden_size))\n",
    "        print(\"num layers: \" + str(num_layers))\n",
    "        print(\"num classes: \" + str(num_classes))\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = embeddings.cuda()\n",
    "        self.rnn_premise = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.rnn_hypothesis = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.linear_premise = nn.Linear(hidden_size, num_classes)\n",
    "        self.linear_hyp = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.linear_fully_connected = nn.Linear(6, 32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(32, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = premises.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Run encoder for the premise\n",
    "        # get embedding of tokens\n",
    "        embed_premises = self.embedding(premises)\n",
    "        # pack padded sequence\n",
    "        embed_premises = torch.nn.utils.rnn.pack_padded_sequence(embed_premises, \n",
    "                                                                 lengths, \n",
    "                                                                 batch_first=True)\n",
    "        # fprop though RNN\n",
    "        rnn_out_prem, self.hidden = self.rnn_premise(embed_premises, self.hidden)\n",
    "        # undo packing\n",
    "        rnn_out_prem, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_prem, \n",
    "                                                                 batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out_prem = torch.sum(rnn_out_prem, dim=1)\n",
    "        logits_prem = self.linear_premise(rnn_out_prem)\n",
    "\n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "#         embed_hyp = torch.nn.utils.rnn.pack_padded_sequence(embed_hyp, \n",
    "#                                                                  lengths.numpy(), \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp, self.hidden = self.rnn_hypothesis(embed_hyp, self.hidden)\n",
    "#         rnn_out_hyp, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_hyp, \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp = torch.sum(rnn_out_hyp, dim=1)\n",
    "        logits_hyp = self.linear_hyp(rnn_out_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes, kernel_size=3):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = embeddings.cuda()\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(embeddings.embedding_dim, hidden_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=1)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool1d(kernel_size, stride=2)\n",
    "        self.linear = nn.Linear( int(np.ceil(hidden_size / 2.0) - 1), 128)\n",
    "        #128 x 256\n",
    "        self.linear_fully_connected = nn.Linear(256, 128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        premises, hypotheses, lengths = premises.cuda(), hypotheses.cuda(), lengths.cuda()\n",
    "        \n",
    "        batch_size, seq_len = premises.size()\n",
    "        \n",
    "        # Run encoder for the premises\n",
    "#         print(\"DEGUB-----\")\n",
    "#         print(premises)\n",
    "#         print(premises.double())\n",
    "#         print(premises.int())\n",
    "#         print(premises.type(torch.cuda.LongTensor))\n",
    "        embed_premises = self.embedding(premises)\n",
    "        hidden_prem = self.conv1(embed_premises.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "\n",
    "        hidden_prem = self.conv2(hidden_prem.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "        \n",
    "        pooled_prem = self.max_pool(hidden_prem)\n",
    "\n",
    "        hidden_prem = torch.sum(pooled_prem, dim=1)\n",
    "        logits_prem = self.linear(hidden_prem)\n",
    "        \n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "        hidden_hyp = self.conv1(embed_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "\n",
    "        hidden_hyp = self.conv2(hidden_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "        \n",
    "        pooled_hyp = self.max_pool(hidden_hyp)\n",
    "\n",
    "        hidden_hyp = torch.sum(pooled_hyp, dim=1)\n",
    "        logits_hyp = self.linear(hidden_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def run_model(mdb, model, accuracies, learning_rate, num_epochs, train_loader, val_loader):\n",
    "    model.cuda()\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    start = time()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (premises, hypotheses, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(premises, hypotheses, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "#             print(loss)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc))\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                end = time()\n",
    "                print(\"Elapsed time {} seconds\".format(round(end - start, 2)))\n",
    "                start = time()\n",
    "                accuracies.append( (epoch+1, i, train_acc, val_acc, model.hidden_size) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdb = ModelDatasetBuilder('hw2_data', vocab_size=50000)\n",
    "mdb.load_fasttext_vectors_into_vocabulary('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50000, 300)\n",
      "All of the lines!!!\n",
      "100000\n",
      "All of the lines!!!\n",
      "1000\n",
      "[[106, 802, 1830, 8, 9, 6265, 7167, 4388, 17, 9, 12229, 5335, 9, 563, 6, 358, 4], [106, 994, 15, 9766, 131, 3, 347, 424, 7, 86, 15, 3356, 17, 9, 2031, 1615, 19, 9, 2607, 17, 21, 4], [17790, 3, 1938, 2, 32, 84, 138, 9, 542, 402, 4], [9, 347, 8, 909, 11907, 5, 9, 884, 7167, 15, 1, 17, 3, 5656], [3543, 884, 3135, 441, 549, 17, 4808, 1826, 5472, 4]]\n",
      "[2, 1, 2, 0, 0]\n",
      "hidden size 25\n",
      "GRU Settings...\n",
      "embeddings: 300\n",
      "hidden size: 25\n",
      "num layers: 2\n",
      "num classes: 3\n",
      "Epoch: [1/10], Step: [101/782], Training Acc: 33.292\n",
      "Epoch: [1/10], Step: [101/782], Validation Acc: 35.0\n",
      "Elapsed time 19.79 seconds\n",
      "Epoch: [1/10], Step: [201/782], Training Acc: 47.266\n",
      "Epoch: [1/10], Step: [201/782], Validation Acc: 45.3\n",
      "Elapsed time 20.94 seconds\n"
     ]
    }
   ],
   "source": [
    "def run_scenario(mbd):\n",
    "    embeddings = mdb.get_embedding_vector()\n",
    "    print(embeddings)\n",
    "    training_vectors, val_vectors = mdb.get_indexed_text_vectors(max_data=1000000)\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    print([x[0] for x in training_vectors][0:5])\n",
    "    print([x[2] for x in training_vectors][0:5])\n",
    "    # print(training_vectors[0].size)\n",
    "    # print(training_vectors[2])\n",
    "    train_dataset = SnliDataset([x[0] for x in training_vectors], \n",
    "                                [x[1] for x in training_vectors], \n",
    "                                [x[2] for x in training_vectors])\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    val_dataset = SnliDataset([x[0] for x in val_vectors], \n",
    "                                [x[1] for x in val_vectors], \n",
    "                                [x[2] for x in val_vectors])\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    num_epochs = 10\n",
    "    hidden_layers = [25, 50, 100, 200, 400]\n",
    "#     hidden_layers = [400]\n",
    "    for layer in hidden_layers:\n",
    "        print(\"hidden size \"+ str(layer))\n",
    "        gru_learning_rate = 1e-3\n",
    "        model_gru = GRU(embeddings=embeddings, \n",
    "                    hidden_size=layer, \n",
    "                    num_layers=2, \n",
    "                    num_classes=3)\n",
    "        accuracies_gru = []\n",
    "        run_model(mdb, model_gru, accuracies_gru, gru_learning_rate, num_epochs, train_loader, val_loader)\n",
    "        pkl.dump(accuracies_gru, open(\"gru_accuracies3_{}_{}\".format(gru_learning_rate,layer), \"wb\"))\n",
    "        print(accuracies_gru)\n",
    "        \n",
    "#         cnn_learning_rate = 3e-4\n",
    "#         model_cnn = CNN(embeddings=embeddings, \n",
    "#                     hidden_size=layer, \n",
    "#                     num_layers=2, \n",
    "#                     num_classes=3)\n",
    "#         accuracies_cnn = []\n",
    "#         run_model(mdb, model_cnn, accuracies_cnn, cnn_learning_rate, num_epochs, train_loader, val_loader)\n",
    "#         pkl.dump(accuracies_cnn, open(\"cnn_accuracies2_{}_{}\".format(cnn_learning_rate,layer), \"wb\"))\n",
    "#         print(accuracies_cnn)\n",
    "\n",
    "run_scenario(mdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this so your plots show properly\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "def get_plot_values(accuracies):\n",
    "    training_acc = [acc[2] for acc in accuracies]\n",
    "    val_acc = [acc[3] for acc in accuracies]\n",
    "    training_acc_fmt = []\n",
    "    for i, acc in enumerate(training_acc):\n",
    "        if i % 6 == 0:\n",
    "            continue\n",
    "        elif i % 7 == 0:\n",
    "            continue\n",
    "        else:\n",
    "            training_acc_fmt.append(acc)\n",
    "    val_acc_fmt = []\n",
    "    for i, acc in enumerate(val_acc):\n",
    "        if i % 6 == 0:\n",
    "            continue\n",
    "        elif i % 7 == 0:\n",
    "            continue\n",
    "        else:\n",
    "            val_acc_fmt.append(acc)\n",
    "    return training_acc_fmt, val_acc_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gru_accuracies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-87e5718d24bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracies_gru_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gru_accuracies\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgru_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_plot_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies_gru_loaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccuracies_cnn_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_accuracies2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnn_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_plot_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies_cnn_loaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gru_accuracies'"
     ]
    }
   ],
   "source": [
    "accuracies_gru_loaded = pkl.load(open(\"gru_accuracies\", \"rb\"))\n",
    "gru_training, gru_val = get_plot_values(accuracies_gru_loaded)\n",
    "accuracies_cnn_loaded = pkl.load(open(\"cnn_accuracies2\", \"rb\"))\n",
    "cnn_training, cnn_val = get_plot_values(accuracies_cnn_loaded)\n",
    "\n",
    "\n",
    "plt.plot(list(np.arange(0, 20, 0.2)), gru_training, label=\"GRU Training Accuracy\", color='purple', linestyle='--') \n",
    "plt.plot(list(np.arange(0, 20, 0.2)), gru_val, label=\"GRU Validation Accuracy\", color='purple') \n",
    "plt.plot(list(np.arange(0, 20, 0.2)), cnn_training, label=\"CNN Training Accuracy\", color='blue', linestyle='--') \n",
    "plt.plot(list(np.arange(0, 20, 0.2)), cnn_val, label=\"CNN Validation Accuracy\", color='blue') \n",
    "\n",
    "\n",
    "plt.title('Comparing Accuracy For Base GRU and CNN models')\n",
    "plt.legend(loc = 'lower right')\n",
    "# plt.ylim([0, 15])\n",
    "plt.xlim([0, 20])\n",
    "plt.xticks(list(range(0,21,2)))\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Number of Epochs')\n",
    "txt=\"* Adam-based optimizer with fixed learning rate=0.0003 and hidden=200\"\n",
    "plt.figtext(0.5, -0.05, txt, wrap=True, horizontalalignment='center', fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies_gru_loaded = pkl.load(open(\"gru_accuracies_0.003_2\", \"rb\"))\n",
    "gru_training, gru_val = get_plot_values(accuracies_gru_loaded)\n",
    "\n",
    "\n",
    "plt.plot(list(np.arange(0, 10, 0.2)), gru_training, label=\"GRU Training Accuracy\", color='purple', linestyle='--') \n",
    "plt.plot(list(np.arange(0, 10, 0.2)), gru_val, label=\"GRU Validation Accuracy\", color='purple') \n",
    "# plt.plot(list(np.arange(0, 20, 0.2)), cnn_training, label=\"CNN Training Accuracy\", color='blue', linestyle='--') \n",
    "# plt.plot(list(np.arange(0, 20, 0.2)), cnn_val, label=\"CNN Validation Accuracy\", color='blue') \n",
    "\n",
    "\n",
    "plt.title('Trying Faster Learning Rate for GRU')\n",
    "plt.legend(loc = 'lower right')\n",
    "# plt.ylim([0, 15])\n",
    "plt.xlim([0, 7])\n",
    "plt.xticks(list(range(0,8,1)))\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Number of Epochs')\n",
    "txt=\"* Adam-based optimizer with fixed learning rate=0.003 and hidden=200\"\n",
    "plt.figtext(0.5, -0.05, txt, wrap=True, horizontalalignment='center', fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gru_learning_rate = 3e-4\n",
    "hidden_layers = [25, 50, 100, 200]\n",
    "for layer in hidden_layers:\n",
    "    accuracies_gru_loaded = pkl.load(open(\"cnn_accuracies2_{}_{}\".format(gru_learning_rate,layer), \"rb\"))\n",
    "    gru_training, gru_val = get_plot_values(accuracies_gru_loaded)\n",
    "    plt.plot(list(np.arange(0, 10, 0.2)), gru_val, label=\"Validation accuracy for {} hidden layers\".format(layer)) \n",
    "\n",
    "\n",
    "plt.title('Trying Faster Learning Rate for GRU')\n",
    "plt.legend(loc = 'lower right')\n",
    "# plt.ylim([0, 15])\n",
    "plt.xlim([0, 10])\n",
    "plt.xticks(list(range(0,11,1)))\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Number of Epochs')\n",
    "txt=\"* Adam-based optimizer with fixed learning rate=0.003 and hidden=200\"\n",
    "plt.figtext(0.5, -0.05, txt, wrap=True, horizontalalignment='center', fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def \n",
    "\n",
    "mdb = ModelDatasetBuilder('hw2_data', vocab_size=50000)\n",
    "mdb.load_fasttext_vectors_into_vocabulary('wiki-news-300d-1M.vec')\n",
    "embeddings = mdb.get_embedding_vector()\n",
    "print(embeddings)\n",
    "training_vectors, val_vectors = mdb.get_indexed_text_vectors()\n",
    "print(training_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
