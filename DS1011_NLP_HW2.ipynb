{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# dtype = torch.FloatTensor\n",
    "dtype_float = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "dtype_long = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary: you can call line.split() to immediately get the tokenized version of \n",
    "# the sentences. Normally you would have to run the text through a proper tokenizer.\n",
    "\"\"\"\n",
    "You should create a subclass for Dataset, but not for Dataloader. \n",
    "You should probably also define a custom collate_fn, \n",
    "but you should also only need to do this once.\n",
    "You will instantiate separate Datasets: e.g. train_dataset and val_dataset. \n",
    "Then you should create a Dataloader around each one: train_dataloader and val_dataloader.\n",
    "For your max length, that should be determined based on your training set \n",
    "(you should pretend you do not have access to your validation set when choosing \n",
    "such \"hyperparameters\", to avoid biasing your results.\n",
    "\n",
    "sent1_vector = rnn(sent1)\n",
    "sent2_vector = rnn(sent2)\n",
    "combined_vector = torch.cat([sent1_vector, sent2_vector], dim=1)\n",
    "\n",
    "nn.Sequential(nn.Linear(x, hidden_size), \n",
    "              nn.ReLU(inplace=True), \n",
    "              nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "You are required to concatenate the representations, not the sentences. \n",
    "The idea is that you will encode each of your sentences through an \n",
    "encoder (CNN or RNN) and extract a fixed-length vector representation \n",
    "of the sentences. You will then concatenate the two representations and \n",
    "feed that through a fully-connected layer.\n",
    "\n",
    "Transfer the string into word vectors using fast text, process \n",
    "them with data loader, feed them separately into the encoder, \n",
    "get the output representation, concat them and then fed that into a \n",
    "fully-connected layer for classification.\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDatasetBuilder(object):\n",
    "    \"\"\"\n",
    "    Use this class to build the datasets for model consumption\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, vocab_size=10000):\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.TKN_OFFSET = 2\n",
    "        self.EMB_SIZE = 300\n",
    "        self.vocab_size = vocab_size\n",
    "        self.id2token = []\n",
    "        self.token2id = None\n",
    "        self.fasttext_emb_map = {}\n",
    "        pass\n",
    "    \n",
    "    def load_fasttext_vectors_into_vocabulary(self, fname):\n",
    "        self.id2token = [None] * (self.vocab_size + self.TKN_OFFSET)\n",
    "        with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n",
    "            n, d = map(int, fin.readline().split())\n",
    "            for i, line in enumerate(fin):\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                self.id2token[i + self.TKN_OFFSET] = tokens[0]\n",
    "                self.fasttext_emb_map[tokens[0]] = list(map(float, tokens[1:]))\n",
    "                if i>self.vocab_size - self.TKN_OFFSET:\n",
    "                    break\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "        self.id2token[self.PAD_IDX] = '<pad>'\n",
    "        self.id2token[self.UNK_IDX] = '<unk>'\n",
    "        self.token2id = dict(zip(self.id2token, range(0,len(self.id2token)))) \n",
    "        self.token2id['<pad>'] = self.PAD_IDX \n",
    "        self.token2id['<unk>'] = self.UNK_IDX\n",
    "        self.fasttext_emb_map['<unk>'] = np.random.rand(1,self.EMB_SIZE).tolist()[0]\n",
    "        self.fasttext_emb_map['<pad>'] = np.zeros( (1,self.EMB_SIZE) ).tolist()[0]\n",
    "    \n",
    "    def get_indexed_data(self, data_path, max_data):\n",
    "        label_mapper = {\n",
    "            \"contradiction\\n\": 0,\n",
    "            \"entailment\\n\": 1,\n",
    "            \"neutral\\n\": 2\n",
    "        }\n",
    "        lines = []\n",
    "        with open(data_path, 'r', newline='\\n') as f:\n",
    "            next(f)\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                parts = line.split('\\t')\n",
    "                premise_tokens =  parts[0].split()\n",
    "                premise_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in premise_tokens]\n",
    "                hypothesis_tokens =  parts[1].split()\n",
    "                hypothesis_indices = [self.token2id[token] if token in self.token2id else self.UNK_IDX for token in hypothesis_tokens]\n",
    "                label =  label_mapper[parts[2]]\n",
    "                lines.append( (premise_indices, hypothesis_indices, label) )\n",
    "                if i > max_data:\n",
    "                    break\n",
    "                i += 1\n",
    "        print(\"All of the lines!!!\")\n",
    "        print(len(lines))\n",
    "        return lines\n",
    "    \n",
    "    def get_embedding_vector(self):\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        pre_trained_embeddings = []\n",
    "        for i in range(0, self.vocab_size):\n",
    "            token = self.id2token[i]\n",
    "            pre_trained_embeddings.append(self.fasttext_emb_map[token])\n",
    "#         weight = torch.FloatTensor(pre_trained_embeddings)\n",
    "        weight = torch.FloatTensor(pre_trained_embeddings).type(dtype_float).cuda()\n",
    "#         weight = weight.long()\n",
    "        embedding = nn.Embedding.from_pretrained(weight, freeze=True).cuda()\n",
    "        return embedding\n",
    "    \n",
    "    def get_indexed_text_vectors(self, max_data=float(\"inf\")):\n",
    "        \"\"\"\n",
    "        Gets the torch.utils.data.Dataset preproccessed version\n",
    "        \"\"\"\n",
    "        if not self.id2token:\n",
    "            raise ValueError('Please run load_fasttext_vectors_into_vocabulary first!')\n",
    "        training_vectors = self.get_indexed_data('hw2_data/snli_train.tsv', max_data)\n",
    "        val_vectors = self.get_indexed_data('hw2_data/snli_val.tsv', max_data)\n",
    "        return training_vectors, val_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "class SnliDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, premises, hypotheses, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.premises = premises\n",
    "        self.hypotheses = hypotheses\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.premises) == len(self.target_list))\n",
    "        assert (len(self.hypotheses) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        premise_idx = self.premises[index][:MAX_SENTENCE_LENGTH]\n",
    "        hypothesis_idx = self.hypotheses[index][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[index]\n",
    "        return [premise_idx, hypothesis_idx, len(premise_idx), len(hypothesis_idx), label]\n",
    "    \n",
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    premise_list = []\n",
    "    hypothesis_list = []\n",
    "    label_list = []\n",
    "    premise_length_list = []\n",
    "    hypothesis_length_list = []\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    for datum in sorted_batch:\n",
    "        label_list.append(datum[4])\n",
    "        premise_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "        hypothesis_length_list.append(MAX_SENTENCE_LENGTH)\n",
    "    # padding\n",
    "    for datum in sorted_batch:\n",
    "        padded_premise_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        premise_list.append(padded_premise_vec)\n",
    "        padded_hypothesis_vec = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        hypothesis_list.append(padded_hypothesis_vec)\n",
    "\n",
    "    collation = [torch.FloatTensor(premise_list).type(dtype_long), \n",
    "                 torch.FloatTensor(hypothesis_list).type(dtype_long), \n",
    "                 torch.FloatTensor(hypothesis_length_list).type(dtype_long), \n",
    "                 torch.FloatTensor(label_list).type(dtype_long)]\n",
    "    return collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of questions\n",
    "\"\"\"\n",
    "- Do I need to instantiate separate versions of the model for each premise and hypothesis\n",
    "both linear and RNN\n",
    "- Do I have to instantiate in the init function or can I do it in the forward for things\n",
    "like the nn.Sequential operator\n",
    "- How do I parallelize it?\n",
    "- How does the conv net work?\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size        \n",
    "        super(GRU, self).__init__()\n",
    "        print(\"GRU Settings...\")\n",
    "        print(\"embeddings: \" + str(embeddings.embedding_dim))\n",
    "        print(\"hidden size: \" + str(hidden_size))\n",
    "        print(\"num layers: \" + str(num_layers))\n",
    "        print(\"num classes: \" + str(num_classes))\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = embeddings.cuda()\n",
    "        self.rnn_premise = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.rnn_hypothesis = nn.GRU(embeddings.embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True)\n",
    "        self.linear_premise = nn.Linear(hidden_size, num_classes)\n",
    "        self.linear_hyp = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.linear_fully_connected = nn.Linear(6, 32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(32, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = premises.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Run encoder for the premise\n",
    "        # get embedding of tokens\n",
    "        embed_premises = self.embedding(premises)\n",
    "        # pack padded sequence\n",
    "        embed_premises = torch.nn.utils.rnn.pack_padded_sequence(embed_premises, \n",
    "                                                                 lengths, \n",
    "                                                                 batch_first=True)\n",
    "        # fprop though RNN\n",
    "        rnn_out_prem, self.hidden = self.rnn_premise(embed_premises, self.hidden)\n",
    "        # undo packing\n",
    "        rnn_out_prem, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_prem, \n",
    "                                                                 batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out_prem = torch.sum(rnn_out_prem, dim=1)\n",
    "        logits_prem = self.linear_premise(rnn_out_prem)\n",
    "\n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "#         embed_hyp = torch.nn.utils.rnn.pack_padded_sequence(embed_hyp, \n",
    "#                                                                  lengths.numpy(), \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp, self.hidden = self.rnn_hypothesis(embed_hyp, self.hidden)\n",
    "#         rnn_out_hyp, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_hyp, \n",
    "#                                                                  batch_first=True)\n",
    "        rnn_out_hyp = torch.sum(rnn_out_hyp, dim=1)\n",
    "        logits_hyp = self.linear_hyp(rnn_out_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = embeddings.cuda()\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(embeddings.embedding_dim, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.linear_fully_connected = nn.Linear(6, 32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_out = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, premises, hypotheses, lengths):\n",
    "        premises, hypotheses, lengths = premises.cuda(), hypotheses.cuda(), lengths.cuda()\n",
    "        \n",
    "        batch_size, seq_len = premises.size()\n",
    "        \n",
    "        # Run encoder for the premises\n",
    "#         print(\"DEGUB-----\")\n",
    "#         print(premises)\n",
    "#         print(premises.double())\n",
    "#         print(premises.int())\n",
    "#         print(premises.type(torch.cuda.LongTensor))\n",
    "        embed_premises = self.embedding(premises)\n",
    "        hidden_prem = self.conv1(embed_premises.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "\n",
    "        hidden_prem = self.conv2(hidden_prem.transpose(1,2)).transpose(1,2)\n",
    "        hidden_prem = F.relu(\n",
    "            hidden_prem.contiguous().view(-1, hidden_prem.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_prem.size(-1))\n",
    "\n",
    "        hidden_prem = torch.sum(hidden_prem, dim=1)\n",
    "        logits_prem = self.linear(hidden_prem)\n",
    "        \n",
    "        # Run encoder for the hypothesis\n",
    "        embed_hyp = self.embedding(hypotheses)\n",
    "        hidden_hyp = self.conv1(embed_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "\n",
    "        hidden_hyp = self.conv2(hidden_hyp.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hyp = F.relu(\n",
    "            hidden_hyp.contiguous().view(-1, hidden_hyp.size(-1))\n",
    "                       ).view(batch_size, seq_len, hidden_hyp.size(-1))\n",
    "\n",
    "        hidden_hyp = torch.sum(hidden_hyp, dim=1)\n",
    "        logits_hyp = self.linear(hidden_hyp)\n",
    "        \n",
    "        # Interact the two sentences\n",
    "        combined_sentences = torch.cat([logits_prem, logits_hyp], dim=1)\n",
    "        out = self.linear_fully_connected(combined_sentences)\n",
    "        out = self.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for premises, hypotheses, lengths, labels in loader:\n",
    "        premises_batch, hypotheses_batch, lengths_batch, label_batch = premises, hypotheses, lengths, labels\n",
    "        outputs = F.softmax(model(premises_batch, hypotheses_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def run_model(mdb, model, accuracies):\n",
    "    model.cuda()\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 10 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (premises, hypotheses, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(premises, hypotheses, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "#             print(loss)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc))\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                end = time.time()\n",
    "                print(\"Elapsed time {} seconds\".format(round(end - start, 2)))\n",
    "                start = time.time()\n",
    "                accuracies.append( (epoch+1, i, train_acc, val_acc) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdb = ModelDatasetBuilder('hw2_data', vocab_size=50000)\n",
    "mdb.load_fasttext_vectors_into_vocabulary('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50000, 300)\n",
      "All of the lines!!!\n",
      "100000\n",
      "All of the lines!!!\n",
      "1000\n",
      "[[106, 802, 1830, 8, 9, 6265, 7167, 4388, 17, 9, 12229, 5335, 9, 563, 6, 358, 4], [106, 994, 15, 9766, 131, 3, 347, 424, 7, 86, 15, 3356, 17, 9, 2031, 1615, 19, 9, 2607, 17, 21, 4], [17790, 3, 1938, 2, 32, 84, 138, 9, 542, 402, 4], [9, 347, 8, 909, 11907, 5, 9, 884, 7167, 15, 1, 17, 3, 5656], [3543, 884, 3135, 441, 549, 17, 4808, 1826, 5472, 4]]\n",
      "[2, 1, 2, 0, 0]\n",
      "GRU Settings...\n",
      "embeddings: 300\n",
      "hidden size: 200\n",
      "num layers: 2\n",
      "num classes: 3\n",
      "Epoch: [1/10], Step: [101/782], Training Acc: 44.111\n",
      "Epoch: [1/10], Step: [101/782], Validation Acc: 42.9\n",
      "Elapsed time 26.39 seconds\n",
      "Epoch: [1/10], Step: [201/782], Training Acc: 47.777\n",
      "Epoch: [1/10], Step: [201/782], Validation Acc: 47.4\n",
      "Elapsed time 26.34 seconds\n",
      "Epoch: [1/10], Step: [301/782], Training Acc: 53.277\n",
      "Epoch: [1/10], Step: [301/782], Validation Acc: 51.1\n",
      "Elapsed time 26.4 seconds\n",
      "Epoch: [1/10], Step: [401/782], Training Acc: 57.801\n",
      "Epoch: [1/10], Step: [401/782], Validation Acc: 57.0\n",
      "Elapsed time 26.33 seconds\n",
      "Epoch: [1/10], Step: [501/782], Training Acc: 58.859\n",
      "Epoch: [1/10], Step: [501/782], Validation Acc: 58.1\n",
      "Elapsed time 26.41 seconds\n",
      "Epoch: [1/10], Step: [601/782], Training Acc: 60.094\n",
      "Epoch: [1/10], Step: [601/782], Validation Acc: 59.4\n",
      "Elapsed time 26.36 seconds\n",
      "Epoch: [1/10], Step: [701/782], Training Acc: 60.419\n",
      "Epoch: [1/10], Step: [701/782], Validation Acc: 58.1\n",
      "Elapsed time 26.37 seconds\n",
      "Epoch: [2/10], Step: [101/782], Training Acc: 61.078\n",
      "Epoch: [2/10], Step: [101/782], Validation Acc: 59.5\n",
      "Elapsed time 28.94 seconds\n",
      "Epoch: [2/10], Step: [201/782], Training Acc: 62.672\n",
      "Epoch: [2/10], Step: [201/782], Validation Acc: 60.9\n",
      "Elapsed time 26.41 seconds\n",
      "Epoch: [2/10], Step: [301/782], Training Acc: 63.222\n",
      "Epoch: [2/10], Step: [301/782], Validation Acc: 61.4\n",
      "Elapsed time 26.37 seconds\n",
      "Epoch: [2/10], Step: [401/782], Training Acc: 63.853\n",
      "Epoch: [2/10], Step: [401/782], Validation Acc: 63.4\n",
      "Elapsed time 26.37 seconds\n",
      "Epoch: [2/10], Step: [501/782], Training Acc: 63.455\n",
      "Epoch: [2/10], Step: [501/782], Validation Acc: 62.5\n",
      "Elapsed time 26.35 seconds\n",
      "Epoch: [2/10], Step: [601/782], Training Acc: 63.884\n",
      "Epoch: [2/10], Step: [601/782], Validation Acc: 62.8\n",
      "Elapsed time 26.35 seconds\n",
      "Epoch: [2/10], Step: [701/782], Training Acc: 64.712\n",
      "Epoch: [2/10], Step: [701/782], Validation Acc: 62.2\n",
      "Elapsed time 26.4 seconds\n",
      "Epoch: [3/10], Step: [101/782], Training Acc: 65.089\n",
      "Epoch: [3/10], Step: [101/782], Validation Acc: 62.9\n",
      "Elapsed time 28.93 seconds\n",
      "Epoch: [3/10], Step: [201/782], Training Acc: 65.601\n",
      "Epoch: [3/10], Step: [201/782], Validation Acc: 62.5\n",
      "Elapsed time 26.36 seconds\n",
      "Epoch: [3/10], Step: [301/782], Training Acc: 66.035\n",
      "Epoch: [3/10], Step: [301/782], Validation Acc: 63.3\n",
      "Elapsed time 26.36 seconds\n",
      "Epoch: [3/10], Step: [401/782], Training Acc: 65.867\n",
      "Epoch: [3/10], Step: [401/782], Validation Acc: 61.7\n",
      "Elapsed time 26.39 seconds\n",
      "Epoch: [3/10], Step: [501/782], Training Acc: 66.408\n",
      "Epoch: [3/10], Step: [501/782], Validation Acc: 63.5\n",
      "Elapsed time 26.35 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-63855ced6630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m             num_classes=3)\n\u001b[1;32m     35\u001b[0m \u001b[0maccuracies_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-bd8b780b49e0>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(mdb, model, accuracies)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {}'.format(\n\u001b[1;32m     49\u001b[0m                            epoch+1, num_epochs, i+1, len(train_loader), train_acc))\n",
      "\u001b[0;32m<ipython-input-27-bd8b780b49e0>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpremises\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mpremises_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpremises\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremises_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/DL/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-21a3ad0ca85f>\u001b[0m in \u001b[0;36msnli_collate_func\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m                                 mode=\"constant\", constant_values=0)\n\u001b[1;32m     58\u001b[0m         \u001b[0mpremise_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_premise_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         padded_hypothesis_vec = np.pad(np.array(datum[1]), \n\u001b[0m\u001b[1;32m     60\u001b[0m                                 \u001b[0mpad_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SENTENCE_LENGTH\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                 mode=\"constant\", constant_values=0)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = mdb.get_embedding_vector()\n",
    "print(embeddings)\n",
    "training_vectors, val_vectors = mdb.get_indexed_text_vectors(max_data=1000000)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "print([x[0] for x in training_vectors][0:5])\n",
    "print([x[2] for x in training_vectors][0:5])\n",
    "# print(training_vectors[0].size)\n",
    "# print(training_vectors[2])\n",
    "train_dataset = SnliDataset([x[0] for x in training_vectors], \n",
    "                            [x[1] for x in training_vectors], \n",
    "                            [x[2] for x in training_vectors])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = SnliDataset([x[0] for x in val_vectors], \n",
    "                            [x[1] for x in val_vectors], \n",
    "                            [x[2] for x in val_vectors])\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model_gru = GRU(embeddings=embeddings, \n",
    "            hidden_size=200, \n",
    "            num_layers=2, \n",
    "            num_classes=3)\n",
    "\n",
    "model_cnn = CNN(embeddings=embeddings, \n",
    "            hidden_size=200, \n",
    "            num_layers=2, \n",
    "            num_classes=3)\n",
    "accuracies_cnn = []\n",
    "run_model(mdb, model_cnn, accuracies_cnn)\n",
    "print(accuracies_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this so your plots show properly\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "\n",
    "training_acc = [acc[2] for acc in accuracies]\n",
    "val_acc = [acc[3] for acc in accuracies]\n",
    "\n",
    "plt.plot(list(range(0, 70)), training_acc, label=\"Training Accuracy\") \n",
    "plt.plot(list(range(0, 70)), val_acc, label=\"Validation Accuracy\") \n",
    "\n",
    "\n",
    "plt.title('Comparing Training Loss Across Vocabulary Sizes')\n",
    "plt.legend(loc = 'lower right')\n",
    "# plt.ylim([0, 15])\n",
    "plt.xlim([0, 70])\n",
    "\n",
    "plt.ylabel('Loss Score*')\n",
    "plt.xlabel('Number of Epochs')\n",
    "txt=\"* Adam-based optimizer with fixed learning rate=0.01 and n=2 (bigrams)\"\n",
    "plt.figtext(0.5, -0.05, txt, wrap=True, horizontalalignment='center', fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def \n",
    "\n",
    "mdb = ModelDatasetBuilder('hw2_data', vocab_size=50000)\n",
    "mdb.load_fasttext_vectors_into_vocabulary('wiki-news-300d-1M.vec')\n",
    "embeddings = mdb.get_embedding_vector()\n",
    "print(embeddings)\n",
    "training_vectors, val_vectors = mdb.get_indexed_text_vectors()\n",
    "print(training_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
